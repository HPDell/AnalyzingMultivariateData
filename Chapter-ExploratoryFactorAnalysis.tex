% !TeX root = ./main.tex
\chapter{探索性因子分析}

\section{简介}

在之前的章节中，我们讨论了主成分分析——一种多元数据降维以及理解变量间关联模式的方法。
在这个章节中，我们讨论\mindex{探索性因子分析}{exploratory factor analysis}，这是一种类似的方法，
基于一种不同的隐含的模型，被称作“\mindex{公因子模型}{common factor model}”。
虽然这两种方法比较类似，但探索性因子分析经常被用于完成和主成分分析相同的目标。
我们这里的目的是强调他们之间概念上的区别。
公因子模型关于数据集中的每个变量是如何计量的有明确的假设。
模型认为每次计量的变化是由几个相对较少的共同因子造成的（即两个或多个变量之间不可观测的共同特性）。
尽管一个变量实际上可能有一个或多个特性因子，统计上不可能将这些特殊变量混淆。
探索性因子分析的目标是发现共同因子（区别于特性因子）并解释它们和观测数据之间的关系。
因此，尽管用于完成探索性因子分析的解决方案和主成分分析类似，但他们背后的模型是不相同的。

在探索性因子分析中，我们让观测到的数据间关联模式决定因子构成。
在第\ref{chapter:ConfirmatoryFactorAnalysis}章中，我们转向验证性数据分析，
利用一些关于因子构成的先验知识，去检验他们是否与数据一致。
在验证性数据分析中，背后的模型与探索性因子分析是相同的，但是求解过程则大不相同。

公因子模型提供了一个明确的框架让我们可以对数据计量属性进行评估。
在这个章节提供的案例中，我们假设特性因子的变化可以单纯地由计量误差解释。
广义地，为了区分特性方差（即特性因子的方差）和误差方差（即每个变量计量误差的方差），计量可靠性的独立评估是很必要的。
如果没有这种独立评估，我们就假设特性因子方差反映了计量误差。
这让我们对计量的可靠性有了一定的了解：误差方差越小，计量越可靠。
第\ref{chapter:ConfirmatoryFactorAnalysis}章中由关于可靠性的形式化概念。

在这章节的编写过程中，我们特别注重使用旋转来提高因子分析解的解释性。
因为探索性因子分析的方向是随意的，有时候选择有简单结构的解才是有意义的
（即在某种意义上最易于解释的一种，由 Thurstone 在1947年提出）。
实际上，在对数据进行主成分分析时旋转也同样可行。
这里所介绍的所有旋转方法都可以应用于主成分分析。

\subsection{潜在应用}

探索性因子分析与大多数主成分分析的应用场景相同；而公因子模型当测量模型的明确假设是适当时更合适。
下面介绍探索性因子分析的两种说明性应用：分析“潜在特征”或“不可观测的特性”、使用因子得分分析依赖性。

\subsubsection{分析“潜在特征”或“不可观测的特性”}

有的时候区分数据变量和它被设计用于计量的概念是非常重要的。
在处理物理特性时，例如长度和重量（测量仪器具有高精度），这种区分是不必要的，因为目标属性几乎可以被完美计量。
但是，当处理态度、信仰、感知以及其他心理学特性时，我们的计量方法是不完美的。

在市场，调查研究员可能对获取某个概念（例如“顾客满意度”）的信息感兴趣，以更好理解这个概念以及企业行为对它的影响。
设计一个单一的调查问题来准确认知如“顾客满意度”这种概念，即使是可能的，也非常困难。
相反的，研究者可能回设计一个包含几个问题的调查问卷，每个问题并不能完美反映“顾客满意度”，但能够反应某一个方面。
使用因子分析，可以找到这些问题背后的共同变量（很可能反映了客户潜在的满意度）并分离出计量中的非系统误差。
从公因子模型中得到的因子得分可以作为后续分析和建模中顾客满意度的一个指数（或多个指数，取决于潜在变量的个数）。

Aaker（1997）使用探索性因子分析研究了品牌特性的几个维度。
为了处理 114 个特性指标（从心理学和市场调研研究中产生的 309 个候选指标中筛选得到的），
Aaker 让受访者对一组10个品牌中的每一个进行 114 个特性指标的 5 级尺度打分（从 1 到 5 ，1 代表完全没有体现，5 代表非常能够体现）。
她使用了四个不同的品牌数据集，每组都有一个焦点品牌（李维斯牛仔）和九个其他的
（都是非常突出的、广为人知的国民品牌，涵盖了多个不同的产品类型），一共 37 个不同的品牌。
Aaker 对打分结果按人数平均（每个品牌在每个指标上都被大约 150 到 160 个受访者打分，除了李维斯被所有人打分）
并对指标间 $ 114 \times 114 $ 维的相关矩阵进行了因子分析。

Aaker 选择了一个五个因子的解，解释了 90\% 的指标差异。
在进行了旋转之后，她将这些因子标记为：
诚意（sincerity，解释了 26.5\% 的差异）、刺激（excitement，25.1\%）、竞争力（competence，17.5\%）、
世故（sophistication，11.9\%）和粗犷（ruggedness，8.8\%）。
表\ref{tab:traits-associated-with-different-dimensions-of-brand-personality}展示了这五个因子的一些最高度相关的指标。
基于这个研究的结果， Aaker 继续构造并验证了 42 项测度用于测量者五个品牌特性的成分。

\begin{table}
    \centering
    \ttabbox[\textwidth]{
        \caption{品牌特性不同维度相关的指标}
        \label{tab:traits-associated-with-different-dimensions-of-brand-personality}
    }{
        \begin{tabular}{lllll}\toprule
            \bfseries Sincerity & 
            \bfseries Excitement & 
            \bfseries Competence & 
            \bfseries Sophistication & 
            \bfseries Ruggedness \\\midrule
            Honest & Daring & Reliable & Glamorous & Tough \\
            Genuine & Spirited & Responsible & Pretentious & Strong \\ 
            Cheerful & Imaginative & Dependable & Charming & Outdoorsy \\ 
            Down-to-earth & Up-to-date & Efficient & Romantic & Masculine \\ 
            Friendly & Cool & Intelligent & Upper class & \\ 
            & & Successful & Smooth & \\\bottomrule
        \end{tabular}
    }
\end{table}

\subsubsection{使用因子得分分析依赖性}

和主成分分析一样，减少维数往往是因子分析的主要目标。
一方面，这么做有助于对数据进行可视化；另一方面，也有助于增加模型简洁性。
例如，在一个关于新的豪华车概念的大型市场调研中，Roberts（1984）调查了 162 个目标消费者关于他们在汽车的认知。
他使用九个维度来计量他们在熟悉的汽车模型的认值：奢华、样式、可靠性、油耗、安全、维修成本、质量、续航以及道路偏好。
他的最终目标是建立一个模型，将认知和偏好联系起来；但是可用于拟合这个模型的自由维度数太有限。
因此他使用因子分析来看看他是否可以找到一组更少数目的潜在公因子可以替代。

Roberts 发现一个双因子解解释了这九个特性中 60\% 的差异。Roberts 然后旋转了解（使用方差最大法）来使得它更容易解释。
指标和因子之间的相关系数被称为\mindex{因子载荷}{factor loadings}，
如表\ref{tab:two-factor-solution-for-roberts-data}所示；相关系数最高的是用下划线标出。
这种模式指示第一个因子（与奢华、样式、安全性、道路偏好等指标高度相关）反映了车的情感诉求，
而第二个指标（与可靠性、油耗、维修成本、质量和续航相关）反映了车的经济性。
Roberts 将这两个因子标记为吸引力和合理性。

\begin{table}
    \begin{floatrow}
        \ttabbox{
            \caption{Roberts 所用数据的双因子解：因子载荷矩阵}
            \label{tab:two-factor-solution-for-roberts-data}
        }{
            \begin{tabular}{lcc}\toprule
                             & Appealing & Sensible                      \\\midrule
                Luxury       & \underline{$0.884$} & $-0.051$            \\ 
                Style        & \underline{$0.748$} & $ 0.153$            \\ 
                Reliability  & $ 0.396$            & \underline{$0.691$} \\ 
                Fuel Economy & $-0.202$            & \underline{$0.786$} \\ 
                Safety       & \underline{$0.720$} & $ 0.172$            \\ 
                Maintenance  & $ 0.149$            & \underline{$0.756$} \\ 
                Quality      & $ 0.501$            & \underline{$0.650$} \\ 
                Durable      & $ 0.386$            & \underline{$0.677$} \\ 
                Performance  & \underline{$0.686$} & $ 0.391$            \\\bottomrule       
            \end{tabular}
        }
        \ttabbox{
            \caption{心理学考试数据的相关矩阵}
            \label{tab:EFA-Correlation-matrix-for-psychological-test-data}
        }{
            \begin{tabular}{lccccc}\toprule
                & $ PARA $ & $ SENT $ & $ WORD $ & $ ADD $ & $ DOTS $      \\ 
                $ PARA $ & $1.000$ &         &         &         &         \\ 
                $ SENT $ & $0.722$ & $1.000$ &         &         &         \\ 
                $ WORD $ & $0.714$ & $0.685$ & $1.000$ &         &         \\ 
                $ ADD  $ & $0.203$ & $0.246$ & $0.170$ & $1.000$ &         \\ 
                $ DOTS $ & $0.095$ & $0.181$ & $0.113$ & $0.585$ & $1.000$ \\ 
                \bottomrule
            \end{tabular}
        }
    \end{floatrow}
\end{table}

使用这个模型， Roberts 计算每个被打分的车模在每个因子得分的平均值，然后使用这些因子得分来回归前面所述的受访者喜好。
使用这两个因子，他能够解释 30\% 的受访者喜好的差异（依据调整 \rsquare）；这两个因子都高度显著。
相反，当 Roberts 使用 9 个指标来回归受访者喜好时，拟合数据的能力仅有轻微的提高（调整 \rsquare 是 33\%），
而他参数估计值的标准差大幅增加，由于指标间的多重共线性（实际上，九个系数中只有两个在 0.05 等级下显著）。
使用这个简洁的因子模型， Roberts 能够评估新概念车的相对定位，并精确预测市场价值。

\section{原理}

\subsection{直观认识}

和主成分分析一样，对因子分析的直观感觉也最好是通过一个简单的例子进行。
考虑 Holzinger 和 Swineford 进行的针对儿童的心理学考试（1939）。
他们对七、八年级的儿童进行了一些不同的考试。
为了使问题简化，我们主要关注下面五个考试：
篇章理解（$ PARA $）、句子完成（$ SENT $）、单词含义（$ WORD $）、加法（$ ADD $）、数点（$ DOTS $）。
我们使用变量 $ X_1 $ 到 $ X_5 $ 表示这五个不同的考试。
他们间的相关系数（基于 145 个儿童组成的样本）如表\ref{tab:EFA-Correlation-matrix-for-psychological-test-data}所示。

在因子分析中，我们假设考试得分可以用一个潜在公因子和几个特殊因子（每个考试一个特殊因子）的函数来描述。
也就是说，例如，我们相信这些学生的考试得分背后都有一个公因子，记为 $ \xi $ 。
这个因子可能反映了每个学生的智力或应试能力。
我们的但因子模型表示，对考试 $ i $ 的得分 $ X_i $ 是因子 $ \xi $ （对所有五个考试来说都是相同的）
和考试 $ i $ 的特有的因子——不妨记为 $ \delta_i $ ——的函数。
在很多可以用于估计公因子模型参数的方法中，所有我们实际可以估计的是唯一方差（或称“唯一性”），即这个特殊因子和测量误差方差的和。
我们假设这个特殊因子方差完全由测量误差引起，反映考试在完美捕获背后共同因子能力的不足。
由于特殊因子仅影响他们特别针对的计量（即 $ \delta_i $ 仅影响考试 $ i $），
我们的后续处理将在假设这些特殊因子 $ \delta $ 互不相关
（即对于所有不同\!
\footnote{译者注：原文中没有明确表示“不同”的 $ i $ 和 $ j $，但此处确实应不相同。
若相同，则 $ \corr(\delta_i,\delta_j) = 1 $ 而不是 $ 0 $ 。}\!
的 $ i $ 和 $ j $，$ \delta_i $ 和 $ \delta_j $之间的相关系数是 0）。
且与潜在的公因子 $ \xi $ 也不相关的假设下进行。
这些都是公因子模型的标准假设。

这个单因子模型可以用图\ref{fig:EFA-path-diagram-of-one-factor-model-with-five-variables}中的示意图所表示。
图中指向每个考试变量的箭头（即观测计量值，用盒形表示）指示测量中变异的贡献源。
这种情况下，每个测量值 $ X_i $ 有两个变异的贡献源（都不可观测）：公因子 $ \xi $ 和一个特殊因子 $ \delta_i $ 。
用公式的形式，可以写作下式
\begin{align}
    \begin{split}
        X_1 & = \lambda_1\xi + \delta_1 \\ 
        X_2 & = \lambda_2\xi + \delta_2 \\ 
        X_3 & = \lambda_3\xi + \delta_3 \\ 
        X_4 & = \lambda_4\xi + \delta_4 \\ 
        X_5 & = \lambda_5\xi + \delta_5 \\ 
    \end{split}
\end{align}
在这些公式中，系数 $ \lambda $ 反映了潜在公因子 $ \xi $ 体现在测量值 $ X $ 中的程度。
假设 $ X $ 和 $ \xi $ 都是标准化的变量（即有零均值和单位方差）， $ X_i $ 的方差可以被分解为：
\begin{equation}
    \var{X_i} = \var{\lambda_i\xi + \delta_i} = \lambda_i^2 + \var{\delta_i} = 1
\end{equation}
由于变量被标准化，$ \lambda_i $ 可以解释为相关系数。
$ \lambda_i^2 $ 可被解释为 $ X_i $ 中的变化在公因子 $ \xi $ 反映的比例，被称作 $ X_i $ 的\mindex{共同度}{communality}。
$ X_i $ 中剩余的变化可由特殊因子 $ \delta_i $ 解释。
如果用 $ \theta_{ii} = \var{\delta_i} $ 来表示特殊因子的方差（我们假设它反映了 $ X_i $ 的计量误差），
那么 $ X_i $ 的共同度等于
\begin{equation}
    1 - \theta_{ii}^2
\end{equation}

如果 $ X_i $ 的共同度接近 1 （即误差方差接近 0），表示 $ X_i $ 是潜在公因子 $ \xi $ 的一个几乎完美的度量。
反之，如果共同度接近 0，表示 $ \xi $ 完全没有体现在 $ X_i $ 中
（如果 $ X_i $ 是一个对学生智力设计较差的考试，这种情况就可能发生），
那么系数 $ \lambda $ 可能接近零，并且几乎 $ X_i $ 的所有方差都由特殊因子 $ \delta_i $ 解释。

\begin{figure}
    \begin{floatrow}
        \ffigbox{
            \begin{tikzpicture}[
                node distance = 5pt and 5pt,
                common/.style = {circle, inner sep=10pt, draw=black},
                test/.style = {rectangle, inner sep=10pt, draw=black},
                specific/.style = {rectangle, inner sep=2pt, draw=none},
                arrow/.style = {->, >=stealth}
            ]
                \node (xi) [common] {$ \xi $};
                \node (X3) [test,below=of xi,yshift=-20pt] {$ X_3 $};
                \node (X2) [test,left=of X3] {$ X_2 $};
                \node (X1) [test,left=of X2] {$ X_1 $};
                \node (X4) [test,right=of X3] {$ X_4 $};
                \node (X5) [test,right=of X4] {$ X_5 $};
                \node (delta1) [specific,below=of X1,yshift=-8pt] {$ \delta_1 $};
                \node (delta2) [specific,below=of X2,yshift=-8pt] {$ \delta_2 $};
                \node (delta3) [specific,below=of X3,yshift=-8pt] {$ \delta_3 $};
                \node (delta4) [specific,below=of X4,yshift=-8pt] {$ \delta_4 $};
                \node (delta5) [specific,below=of X5,yshift=-8pt] {$ \delta_5 $};
                \draw [arrow] (xi) -- (X1.north);
                \draw [arrow] (xi) -- (X2.north);
                \draw [arrow] (xi) -- (X3.north);
                \draw [arrow] (xi) -- (X4.north);
                \draw [arrow] (xi) -- (X5.north);
                \draw [arrow] (delta1) -- (X1.south);
                \draw [arrow] (delta2) -- (X2.south);
                \draw [arrow] (delta3) -- (X3.south);
                \draw [arrow] (delta4) -- (X4.south);
                \draw [arrow] (delta5) -- (X5.south);
            \end{tikzpicture}
        }{
            \caption{五变量单因子模型线划示意图}
            \label{fig:EFA-path-diagram-of-one-factor-model-with-five-variables}
        }
        \ffigbox{
            \begin{tikzpicture}[
                node distance = 5pt and 5pt,
                common/.style = {circle, inner sep=10pt, draw=black},
                test/.style = {rectangle, inner sep=10pt, draw=black},
                specific/.style = {rectangle, inner sep=2pt, draw=none},
                anchore/.style = {rectangle, inner sep=2pt, draw=none},
                arrow/.style = {->, >=stealth}
            ]
                \node (a_xi) [anchore] {};
                \node (xi1) [common,left=of a_xi] {$ \xi_1 $};
                \node (xi2) [common,right=of a_xi] {$ \xi_2 $};
                \node (X3) [test,below=of a_xi,yshift=-40pt] {$ X_3 $};
                \node (X2) [test,left=of X3] {$ X_2 $};
                \node (X1) [test,left=of X2] {$ X_1 $};
                \node (X4) [test,right=of X3] {$ X_4 $};
                \node (X5) [test,right=of X4] {$ X_5 $};
                \node (delta1) [specific,below=of X1,yshift=-8pt] {$ \delta_1 $};
                \node (delta2) [specific,below=of X2,yshift=-8pt] {$ \delta_2 $};
                \node (delta3) [specific,below=of X3,yshift=-8pt] {$ \delta_3 $};
                \node (delta4) [specific,below=of X4,yshift=-8pt] {$ \delta_4 $};
                \node (delta5) [specific,below=of X5,yshift=-8pt] {$ \delta_5 $};
                \draw [arrow] (xi1) -- (X1.north);
                \draw [arrow] (xi2) -- (X1.north);
                \draw [arrow] (xi1) -- (X2.north);
                \draw [arrow] (xi2) -- (X2.north);
                \draw [arrow] (xi1) -- (X3.north);
                \draw [arrow] (xi2) -- (X3.north);
                \draw [arrow] (xi1) -- (X4.north);
                \draw [arrow] (xi2) -- (X4.north);
                \draw [arrow] (xi1) -- (X5.north);
                \draw [arrow] (xi2) -- (X5.north);
                \draw [arrow] (delta1) -- (X1.south);
                \draw [arrow] (delta2) -- (X2.south);
                \draw [arrow] (delta3) -- (X3.south);
                \draw [arrow] (delta4) -- (X4.south);
                \draw [arrow] (delta5) -- (X5.south);
            \end{tikzpicture}
        }{
            \caption{五变量双因子模型线划示意图}
            \label{fig:EFA-path-diagram-of-two-factor-model-with-five-variables}
        }
    \end{floatrow}
\end{figure}

在这个特殊的示例中，
我们从一个潜在于 Holzinger 和 Swineford 的研究中考试得分的单共同因子假设开始（一种类似于通用智力的东西）。
但是，也有一种情况，即考试中的个人表现可能是多于一个潜在能力的函数。
例如，我们可能认为学生的考试表现有两个不同的内在能力所决定——\!
文学天赋（记为 $ \xi_1 $）和计量天赋（记为 $ \xi_2 $）——\!
并且这两种不同的因子在不同类型的考试中有不同程度的表现。
这个双因子模型如图\ref{fig:EFA-path-diagram-of-two-factor-model-with-five-variables}所示。

在双因子模型中，在每个考试得分度量 $ X_i $ 中有三个源贡献了变异：
两个公因子 $ \xi_1 $ 和 $ \xi_2 $ 以及一个特殊变量 $ \delta_i $。
使用公式的形式，双因子模型可以写作：
\begin{align}
    \begin{split}
        X_1 & = \lambda_{11}\xi_1 + \lambda_{12}\xi_2 + \delta_1 \\
        X_2 & = \lambda_{21}\xi_1 + \lambda_{22}\xi_2 + \delta_2 \\
        X_3 & = \lambda_{31}\xi_1 + \lambda_{32}\xi_2 + \delta_3 \\
        X_4 & = \lambda_{41}\xi_1 + \lambda_{42}\xi_2 + \delta_4 \\
        X_5 & = \lambda_{51}\xi_1 + \lambda_{52}\xi_2 + \delta_5 \\
    \end{split}
\end{align}

如前，系数 $ \lambda $ 反映了潜在公因子 $ \xi $ 体现在测量值 $ X $ 中的程度。
如果 $ X $ 和 $ \xi $ 被标准化，参数 $ \lambda $ 可以被解释为相关系数。
如果我们进一步假设潜在因子互不相关（公因子模型的另一个标准假设），
那么每个考试度量的共同度由那个变量因子载荷的平方和给出；
因此，$ X_i $ 的共同度由 $ \lambda_{11}^2 + \lambda_{12}^2 $ 给出。

考虑在双因子模型下，如果一个学生有较高的文学天赋（高 $ \xi_1 $）和较低的计量天赋（低 $ \xi_2 $），那会发生什么。
我们可以预期这个学生在那些比计量天赋更需要文学天赋的考试中表现良好。
如果学生在句子完成考试中的表现只依赖于他（她）的文学天赋，那么我们应该期望一个
接近于 1 的 $ \lambda_{11} $ 的值和一个接近于 0 的 $ \lambda_{12} $ 的值。

在没有较强的关于学生考试表现潜在因子结构的先验知识这个探索性设定中，
我们希望能够从数据中能够推到出潜在因子的合理数量以及公因子模型公式中的系数值。
这个方法（在下一节中会详细阐述）和主成分分析的方法相类似，都是尝试提取一个较少数量的因子组以充分代表观测值的相关矩阵。
不同的是，公因子模型中，我们必须解释特殊因子 $ \delta $ ，而这没有在主成分分析中体现。

\subsection{求解过程}

和主成分分析一样，探索性因子分析的求解过程主要关注于 $ \vX $ 的协方差矩阵或相关系数矩阵的分解。
这两种方法的不同之处在于，特殊因子是公因子模型的一部分。
由于这些特殊因子被认为彼此不相关，而且与潜在公因子独立，这些因子仅仅贡献于协方差矩阵的对角线元素。
这一点通过检查每个度量值 $ X_i $ 协方差矩阵的对角线元素可以很容易发现：
\begin{equation}
    \var{X_i} = \var{\lambda_{i1}\xi_1 + \lambda_{i2}\xi_2 + \delta_i}
\end{equation}

$ X_i $ 方差表达式中有九个交叉项。
根据公因子模型加入的独立性假设（以及进一步的公因子被标准化为具有单位方差的假设），
所有的协方差项都排除在模型之外，只剩
\begin{equation}
    \var{X_i} = \lambda_{i1}^2 + \lambda_{i2}^2 + \theta_{ii}^2
\end{equation}
其中 $ \theta_{ii}^2 = \var{\delta_i} $ 。
因此，特殊因子出现在 $ \vX $ 的协方差矩阵中的唯一位置就是对角线，也就是他们对每个度量贡献变异的地方。

如果我们实现直到每个特殊因子的变异呢？
如果这样，那么我们可以在协方差矩阵中减去这些值。
然后我们剩下一个矩阵，仅由潜在公因子的方差和协方差构成。
然后我们可以使用主成分分析方法来分解这些矩阵，并寻找公因子。
这是我们在探索性因子分析中使用的求解过程的本质。
在主成分分析中，我们分解相关矩阵 $ \vR $ （对角线元素都是 1）。
但是对于公因子模型，我们分解对角线元素为 $ 1 - \theta_{ii}^2 $ 的相关矩阵。
实际上，我们通过在模型中减去由于特殊因子引起的变异开始
（回想一下，这些变异可以被解释为测量误差或其他针对于不同考试的变异源，而且和其他因子不相关），
仅在对角线上保留共同度。
然后我们尝试使用公因子解释剩下的变异。
如果我们知道共同度，那么我们就可以使用与主成分分析相同的方法解决这个问题。
这有时被称为\mindex{主轴因子化}{principal axis factoring}（或简单的被称为公因子模型的主成分方法）。
共同度的估计是这个求解过程中非平凡的部分，有许多不同的方法。

现在让我们假设我们不知道在我们这个解释性示例中由于特殊因子引起的变异的大小
（在后续章节中，我们讨论当我们不知道这些值时该怎么办）。
对于这个考试集合，我们假设每个考试中大约一半的变异都有特殊因子产生，
也就是说，我们开始时对所有 $ i $ 设置 $ \theta_{ii}^2 = 0.50 $ 
然后对这个修改的矩阵进行矩阵分解。
特征值如下所示：
\begin{equation}
    \begin{array}{ccccc}
        \lambda_1 = 2.187 &
        \lambda_2 = 1.022 &
        \lambda_3 = -0.135 &
        \lambda_4 = -0.089 &
        \lambda_5 = 0.015 
    \end{array}
\end{equation}

首先值得注意的是，这些特征值和主成分分析的特征值有所不同。
并不是所有的特征值都是正的，并且他们的和也不是 $ p $ （分析中变量的数目）。
原因是，通过减去由于特殊因子引起的变异，我们减少了需要由公因子解释的剩余的信息。
这个模型中已经由特殊因子解释的变异是 $ 0.50 + 0.50 + 0.50 + 0.50 + 0.50 = 2.5 $，
即 $ 2.5/5.0 = 50\% $ 的原始五个考试得分的变异。
对角线元素的和也是 $ 2.5 $，即由公因子解释的方差。

现在仍然存在问题：我们需要多少公因子？
注意到选择提取的因子数量的标准与主成分分析相比多少有点不同变化。
由于有一些解释了一部分变异的特殊因子，我们的目标是使用公因子解释尽可能多的剩余变异。
因此我们寻找有意义地不同于零地特征值。
在这种情况下，是 2，表示我们提取 $ c = 2 $ 个公因子。

双因子解的\mindex{因子载荷}{factor loadings}矩阵（即有原始变量和公因子地相关系数组成地矩阵）
如表\ref{tab:EFA-matrix-of-factor-loadings-for-two-factor-model}所示。
这种因子结构的模式表现出前三个考试（篇章理解、句子完成、单词含义）在第一个公因子上载荷更多（所有相关系数都接近 0.8），
而后两个考试（加法和计数）在第二个公因子上载荷更高（所有相关系数都接近 0.6）。
这和第一个因子反映学生的文学天赋、第二个因子指示计量天赋的解释一致。


\begin{table}
    \centering
    \begin{floatrow}
        \ttabbox[\FBwidth]{
            \caption{双因子模型的因子载荷矩阵（近似共同度）}
            \label{tab:EFA-matrix-of-factor-loadings-for-two-factor-model}
        }{
            \begin{tabular}{lrr}\toprule
                & Factor 1 & Factor 2 \\\midrule
                $ PARA $ & $0.7722$ & $-0.2351$ \\ 
                $ SENT $ & $0.7838$ & $-0.1576$ \\ 
                $ WORD $ & $0.7562$ & $-0.2372$ \\ 
                $ ADD  $ & $0.4293$ & $ 0.6017$ \\ 
                $ DOTS $ & $0.3476$ & $ 0.6506$ \\ 
                \bottomrule
            \end{tabular}
        }
        \ttabbox[\FBwidth]{
            \caption{使用多元相关平方系数作为初始估计值进行的因子分析}
            \label{tab:EFA-factor-analysis-with-smcs-as-initial-estimates}
        }{
            \begin{tabular}{l}\toprule
                \begin{minipage}{0.5\textwidth}
                    \begin{tabular}{ccccc}
                        \multicolumn{5}{c}{\bfseries 先验共同度估计：多元相关平方系数} \\\midrule
                        $ PARA $ & $ SENT $ & $ WORD $ & $ ADD  $ & $ DOTS $ \\ 
                        $0.6158$ & $0.5914$ & $0.5701$ & $0.3672$ & $0.3493$ \\
                    \end{tabular}
                \end{minipage} \\\bottomrule
                \begin{minipage}{0.5\textwidth}
                    \begin{tabular}{ccccc}
                        \multicolumn{5}{c}{\bfseries 最终特征值} \\\midrule
                        1 & 2 & 3 & 4 & 5 \\ 
                        $2.2826$ & $1.0273$ & $0.0252$ & $-0.0010$ & $-0.0247$ \\ 
                    \end{tabular}
                \end{minipage} \\\bottomrule
                \begin{minipage}{0.5\textwidth}
                    \begin{tabular}{lrr}
                        & \multicolumn{2}{c}{\bfseries 因子模式} \\\cmidrule{2-3}
                        & Factor 1 & Factor 2 \\ 
                        $ PARA $ & $0.8349$ & $-0.2418$ \\
                        $ SENT $ & $0.8253$ & $-0.1398$ \\
                        $ WORD $ & $0.7898$ & $-0.2274$ \\
                        $ ADD  $ & $0.4146$ & $ 0.6503$ \\
                        $ DOTS $ & $0.3297$ & $ 0.6890$ \\
                        & \multicolumn{2}{c}{每个因子解释的方差} \\
                        & $2.2826$ & $1.0273$ \\ 
                    \end{tabular}
                \end{minipage}\\\bottomrule
            \end{tabular}
        }
    \end{floatrow}
\end{table}

使用因子载荷，我们也可以通过这两个公因子计算每项考试解释的变异所占的比例。
这个比例被称为\mindex{共同度}{communality}。
例如，对于篇章理解考试（$ X_1 $），公因子解释了考试中 $ (0.77)^2 + (-0.24)^2 = 0.65 $ 即 65\% 的变异。
这个共同度值比我们一开始在分析中使用的 $ 1 - \theta_{11}^2 = 0.50 $ 的值略高。
这是因为我们的起点是基于一些先验知识，只是近似的。
对于学生的特定样本，精确值更可能是不同的值。

通过跌多过程来修正我们初始估计值是可行的，最终我们会得到一个一致的结果。
我们用第一次因子分析的结果替代我们处是共同度估计值，并再次进行因子分析。
那么，我们将使用新的值 0.65 代替 $ X_1 $ （篇章理解） 对应的对角线上的元素 0.50，
对于其他变量我们也这么做。
我们可以继续一次迭代、两次迭代这个过程，直到它收敛——也就是说，直到两次迭代之间共同度的变化足够小。
这个迭代过程经常被用于共同度的估计。
这个方法有时被称为\mindex{主因子法}{principal factor method}。

如果我们没有足够的关于我们数据中测量误差的先验知识（即我们的测量中与潜在因子无关的非系统变异）呢？
我们使用什么来设置共同度的初始估计值？
一个被广泛使用的方法是\mindex{多元相关平方系数}{squared multiple correlation}，
即在数据集中一个变量中可被其他所有变量解释的方差的大小。
例如，如果我们想使用多元相关平方系数作为作为 $ X_1 $ （篇章理解）的初始估值，
我们将 $ X_1 $ 与其余的变量 $ X_2, X_3, X_4, X_5 $ 进行回归，并使用 \rsquare 值。

为什么多元相关平方系数是共同度的良好估值？
因为共同度是由公因子 $ \xi $ 所解释的 $ X $ 中方差的占比。
尽管我们因此喜欢用公因子 $ \xi $ 对 $ X $ 进行回归，但也仍然存在公因子不可被观测的问题。
但是我们有其余变量 $ X $，每个都反映了潜在变量 $ \xi $（尽管并不完美）。
因为测量有误差，他们解释 $ X $ 的能力被减弱了。
因此，多元相关平方系数可以作为共同度的下界。
一般地，测量越可靠（即特殊因子有较小的方差），多元相关平方系数作为共同度的估值越准确。

表展示了考试得分数据使用多元相关平方系数作为共同度估值的因子分析结果。
我们使用一个像上述的迭代过程：用修正的共同度估值代替初始值并继续，直到后续的共同度估计值没有明显改变。

表\ref{tab:EFA-matrix-of-factor-loadings-for-two-factor-model}所示的初始的因子载荷和
表\ref{tab:EFA-factor-analysis-with-smcs-as-initial-estimates}所示的最终结果的差异并不显著。
因子模式指向相同的因子本质的解释。
变异的划分也基本相同：由共同因子解释的变异的大小仍然大约是 66\%。
对比两个初始估计值，最后两个考试（加法和计数）相关的共同度比前三个低：
对于 $ X_4 $ 和 $ X_5 $，共同度大约是 0.60，而对于 $ X_1 $，$ X_2 $ 和 $ X_3 $ 共同度大约是 0.70。
如果我们认为这种差异来自于测量误差，那么我们可能得出后两个考试比前两个考试更不可靠的结论。

关于这种共同度的估计方法，需要注意一点。
如果技术考试被省略，仅保留加法作为学生计量天赋的唯一度量，会怎么样？
在这些情况下，基于多元相关平方系数计算的假发考试的共同度的估值会非常低
（因为前三个考试没有任何一个反映第二个共同因子）。
因此，我们更可能得出加法是一项非常不可靠的考试，但实际上它是一个重要潜在结构的唯一度量。
正是共同度估值的这个问题，导致有人更喜欢主成分分析法而不是因子分析法。

\subsubsection{旋转因子解}

在下面第\ref{sec:EFA-Mechanics}节中我们会看到，公因子模型实际上拥有无穷多解，
每种解都有平等的能力再现观测到的协方差矩阵。
原因是因子解的方向（即描述坐标系统的基向量的选择）是非常随意的。
这被称为公因子模型的\mindex{旋转不确定性}{rotational indeterminacy}。
在主成分分析中，为了避免不确定性，我们将问题这样定义：
第一主成分是原始数据（经过合理缩放）具有最大方差的线性组合，
第二主成分是拥有次大方差（条件是与第一主成分不相关）的线性组合，等等。
这保证了唯一（尽管有些随意）解。

如果因子解的方向是非常随意的，那么为什么不选择一个可以更好地帮助我们理解并解释数据的解？
使用因子载荷矩阵尝试提出潜在公因子的一个明确解释往往是比较头疼的。
如果我们可以通过旋转因子解选择一个不同的方向使载荷矩阵得到简化则是非常有利的。
问题就是，如何选择这个方向？
如何使变化因子载荷矩阵的目标变得可操作，使我们可以找到让我们更接近那个目标的因子解的旋转方式？

寻找这个旋转矩阵 $ \vT $（我们暂时将讨论限制在正交旋转矩阵范围内，这保留了潜在公因子的独立性）最流行的方法
是基于 Thurstone （1947）描述简单结构原则。
Thurstone 认为大多数内容域可能涉及几个隐藏的（即潜在的或不可观测的）因子。
他同样假设任何一个观测到的变量可能和一个或几个潜在因子相联系，而且任何一个因子可能仅和几个变量相联系。
那么最一般的想法，只要可能，就是找到几个变量簇，每个簇仅明确一个因子。
更一般地，我们想要找到几个因子轴的方向，使得每项考试（或其他变量）尽在几个因子上有相对较高的载荷（正或负），
而其他因子的载荷趋于零。

如果潜在简单结构的论证有效，那么因子载荷矩阵应该展现出一种特定的模式（Comery，1973）：
\begin{enumerate}
    \item 大多数特殊因子（列）的载荷应当很小（尽可能接近于零），仅仅一些载荷的绝对值应该很大。
    \item 载荷矩阵的某行，包括每个因子的给定变量的载荷，应当在一个或少数因子上表现出非零载荷。
    \item 任何一对因子（列）应当展现出不同的载荷模式。否则，这两列所表示的因子无法区分。
\end{enumerate}

一个可以表明简单结构概念的假设例子如
表\ref{tab:EFA-factor-loadings-for-pain-relievers}和
图\ref{fig:EFA-plot-of-factor-loadings-for-attributes-of-pain-relievers}所示。
设想一个对消费者对止痛药的看法的研究，其中要求每个受试者根据六个属性评价他（她）的首选止痛药品牌：
\begin{enumerate}
    \item 不反胃
    \item 没有副作用
    \item 能止痛
    \item 起效快
    \item 能使人保持清醒
    \item 适度依赖性\footnote{原文：Provides limited relief.}
\end{enumerate}

\begin{table}[htb]
    \ttabbox[\textwidth]{
        \caption{止痛药因子载荷矩阵}
        \label{tab:EFA-factor-loadings-for-pain-relievers}
    }{
        \begin{tabularx}{\textwidth}{XZZZZZ}\toprule
            & \multicolumn{2}{c}{\textbf{未旋转解}} & & \multicolumn{2}{c}{\textbf{旋转的解}} \\\cmidrule{2-3}\cmidrule{5-6}
            特性           & Factor 1 & Factor 2 & & Factor 1 & Factor 2 \\ 
            不反胃         & $ 0.579$ & $-0.452$ & & $ 0.139$ & $ 0.721$ \\ 
            没有副作用     & $ 0.522$ & $-0.572$ & & $ 0.017$ & $ 0.774$ \\ 
            能止痛         & $ 0.645$ & $ 0.436$ & & $ 0.772$ & $ 0.097$ \\ 
            起效快         & $ 0.542$ & $ 0.542$ & & $ 0.764$ & $-0.051$ \\ 
            能使人保持清醒 & $-0.476$ & $ 0.596$ & & $-0.034$ & $-0.762$ \\ 
            适度依赖性     & $-0.613$ & $-0.439$ & & $-0.750$ & $-0.074$ \\
            & \multicolumn{2}{c}{解释的方差} & & \multicolumn{2}{c}{解释的方差} \\ 
            & $1.921$ & $1.562$ & & $1.765$ & $1.718$ \\\bottomrule 
        \end{tabularx}
    }
\end{table}

\begin{figure}[htb]
    \begin{floatrow}
        \ffigbox[\textwidth]{
            \begin{subfloatrow}
                \ffigbox{
                    \begin{tikzpicture}[scale=3.5]
                        \def\cross#1{
                            \draw [line width=1pt] ($(#1)+(-0.03,0)$)--($(#1)+(0.03,0)$);
                            \draw [line width=1pt] ($(#1)+(0,-0.03)$)--($(#1)+(0,0.03)$);
                        }
                        \draw (-1, 0) node[label=left:$0.0$]{} -- ( 1, 0);
                        \draw ( 0,-1) node[label=below:$0.0$]{} -- ( 0, 1);
                        \draw (-1,-1) -- (-1, 1) -- ( 1, 1) -- ( 1,-1) -- cycle;
                        \coordinate (O) at (0,0);
                        \coordinate [label=45:不反胃]                    (X1) at ( 0.579,-0.452);
                        \coordinate [label=below:没有副作用]             (X2) at ( 0.522,-0.572);
                        \coordinate [label={[below,xshift=15pt]:能止痛}] (X3) at ( 0.645, 0.436);
                        \coordinate [label=above:起效快]                 (X4) at ( 0.542, 0.542);
                        \coordinate [label=above:能使人保持清醒]         (X5) at (-0.476, 0.596);
                        \coordinate [label=below:适度依赖性]             (X6) at (-0.613,-0.439);
                        \draw (O) -- (X1); \cross{X1};
                        \draw (O) -- (X2); \cross{X2};
                        \draw (O) -- (X3); \cross{X3};
                        \draw (O) -- (X4); \cross{X4};
                        \draw (O) -- (X5); \cross{X5};
                        \draw (O) -- (X6); \cross{X6};
                    \end{tikzpicture}
                }{\caption{未旋转解}}
                \ffigbox{
                    \begin{tikzpicture}[scale=3.5]
                        \def\cross#1{
                            \draw [line width=1pt] ($(#1)+(-0.03,0)$)--($(#1)+(0.03,0)$);
                            \draw [line width=1pt] ($(#1)+(0,-0.03)$)--($(#1)+(0,0.03)$);
                        }
                        \draw (-1, 0) node[label=left:$0.0$]{} -- ( 1, 0);
                        \draw ( 0,-1) node[label=below:$0.0$]{} -- ( 0, 1);
                        \draw (-1,-1) -- (-1, 1) -- ( 1, 1) -- ( 1,-1) -- cycle;
                        \coordinate (O) at (0,0);
                        \coordinate [label=right:不反胃]                    (X1) at ( 0.139, 0.721);
                        \coordinate [label=left:没有副作用]                 (X2) at ( 0.017, 0.774);
                        \coordinate [label=above:能止痛]                    (X3) at ( 0.772, 0.097);
                        \coordinate [label=below:起效快]                    (X4) at ( 0.764,-0.051);
                        \coordinate [label=left:能使人保持清醒]             (X5) at (-0.034,-0.762);
                        \coordinate [label={[below,xshift=5pt]:适度依赖性}] (X6) at (-0.750,-0.074);
                        \draw (O) -- (X1); \cross{X1};
                        \draw (O) -- (X2); \cross{X2};
                        \draw (O) -- (X3); \cross{X3};
                        \draw (O) -- (X4); \cross{X4};
                        \draw (O) -- (X5); \cross{X5};
                        \draw (O) -- (X6); \cross{X6};
                    \end{tikzpicture}
                }{\caption{旋转解}}   
            \end{subfloatrow}
        }{
            \caption{止痛药特性因子载荷图：未旋转的和旋转的}
            \label{fig:EFA-plot-of-factor-loadings-for-attributes-of-pain-relievers}
        }
    \end{floatrow}
\end{figure}

在表\ref{tab:EFA-factor-loadings-for-pain-relievers}中第一部分所示的因子载荷矩阵是未经旋转的双因子分析的解。
（回想一下，这些未旋转的因子通过使第一个公因子解释最多变异、
第二个公因子在与第一个公因子物馆的条件下尽可能解释更多剩下的变异而定向。）
注意到所有载荷矩阵的 12 个元素都相对较大（绝对值都大于 0.4）。
有这么多重大交叉载荷，获得因子的简单解释就变得比较困难。

图\ref{fig:EFA-plot-of-factor-loadings-for-attributes-of-pain-relievers}绘制出因子载荷，
并展示旋转因子空间使特性更接近地位于公因子附近如何成为可能。
想法是选择一个旋转角，使每个特性在公因子上地投影要么很大（即绝对值将近 1）要么很小（绝对值接近 0）。
这实际上减小了交叉载荷并在简单结构方向上移动了载荷矩阵。

表\ref{tab:EFA-factor-loadings-for-pain-relievers}所示的旋转的因子载荷矩阵展示了接近简单结构的东西。
特性 3 和 4（“能止痛”和“起效快”）在第一个公因子上有正载荷，特性 6（适度依赖性）有负载荷。
特性 1 和 2（“不反胃”和“没有副作用”）在第二个公因子上有正载荷，特性 5（“能保持清醒”）有负载荷。
所有其他的载荷的绝对值都很小。
给定定义每个因子的属性簇，我们可以标记第一个因子为“有效性”和第二个因子“温和性”。
（负载荷反映负相关。例如，“能保持清醒”的得分越高，“温和性”得分越低。）

需要注意的是，旋转过后，初始因子方向方差最大化的性质就丢失了；
也就是说，虽然总体上保留的因子能够解释与原先的数据集中一样多的变异，但这种变异现在在旋转配置的新维度上有所不同。
因此，第一个（旋转后的）因子解释最大的变异的情况不存在了。
由于每个因子解释的变异的大小通常并不是关心的主要目标，如果旋转后的解更容易解释，那么这种取舍通常认为是值得的。

\subsection{严密推导}\label{sec:EFA-Mechanics}

在这个更形式化的因子分析模型的处理中，回到主成分分析模型的建立过程中是非常有用的。
我们已经介绍了，对主成分分析问题的求解等同于对标准化的数据矩阵 $ \vX $ 进行奇异值分解，如下所示：
\begin{equation}\label{equ:EFA-x-svd}
    \vX = \vZ_{\symrm{s}} \vD^{\frac{1}{2}}\vU\T
\end{equation}
其中 $ \vZ_{\symrm{s}} $ 是标准化的主成分（全都互不相关），
$ \vD^{\frac{1}{2}} $ 是对角线元素全为主成分标准差的对角矩阵，
$ \vU $ 是特征向量矩阵（全都互相正交）。
据此，我们可以将样本相关矩阵 $ \vR $ 重写为奇异值分解得到的特征值和特征向量的函数。
相关系数矩阵为
\begin{equation}\label{equ:EFA-R-define}
    \vR = \frac{1}{n-1} \vX\T \vX
\end{equation}
带入式(\ref{equ:EFA-x-svd})中的奇异值分解结果到式(\ref{equ:EFA-R-define})中并简化可得
\begin{align}
    \begin{split}
        \vR & = \frac{1}{n-1}(\vZ_{\symrm{s}} \vD^{\frac{1}{2}}\vU\T)\T(\vZ_{\symrm{s}} \vD^{\frac{1}{2}}\vU\T) \\ 
        & = \frac{1}{n-1}\vU\vD^{\frac{1}{2}}(\vZ_{\symrm{s}}\T\vZ_{\symrm{s}})\vD^{\frac{1}{2}}\vU\T \\
        & = (\vU\vD^{\frac{1}{2}})(\vU\vD^{\frac{1}{2}})\T
    \end{split}
\end{align}
因为 $ \hfrac{1}{(n-1)}\vZ_{\symrm{s}}\T\vZ_{\symrm{s}} $ 正好是一个单位阵。
如果我们继续回忆，矩阵乘积 $ \vU\vD^{\frac{1}{2}} $ 正好是因子载荷矩阵 $ \vF $
（即原始数据矩阵 $ \vX $ 和主成分矩阵 $ \vZ $ 的相关系数矩阵），
我们可以 $ \vR $ 的表达式简化为如下形式：
\begin{equation}
    \vR = \vF\vF\T
\end{equation}

由于我们使用主成分分析的目标往往是降维，所以我们尝试提取由 $ c $ 个成分组成的子集
（其中 $ c < p $，$ p $ 是 $ \vX $ 中变量的个数）以近似 $ \vR $。
因此，在主成分中有
\begin{equation}\label{equ:EFA-R-pca}
    \vR \approx \vF_c\vF_c\T
\end{equation}
其中 $ \vF_c $ 仅由因子载荷矩阵 $ \vF $ 的前几列组成。

在探索性因子分析中，我们同样尝试近似相关矩阵 $ \vR $，但是使用一个不同的模型。
在式\eqref{equ:EFA-R-define}中不是将 $ \vX $ 的奇异值分解代入，而是使用上述的公因子模型。
有 $ c $ 个公因子的一般化模型写作：
\begin{align}
    \begin{split}
        X_1 & = \lambda_{11}\xi_{1} + \lambda_{12}\xi_{2} + \cdots + \lambda_{1c}\xi_{c} + \delta_1 \\
        X_2 & = \lambda_{21}\xi_{1} + \lambda_{22}\xi_{2} + \cdots + \lambda_{2c}\xi_{c} + \delta_2 \\
        X_3 & = \lambda_{31}\xi_{1} + \lambda_{32}\xi_{2} + \cdots + \lambda_{3c}\xi_{c} + \delta_3 \\
        & \vdots \\
        X_p & = \lambda_{p1}\xi_{1} + \lambda_{p2}\xi_{2} + \cdots + \lambda_{pc}\xi_{c} + \delta_p \\
    \end{split}
\end{align}
使用矩阵的形式，公因子模型表示为：
\begin{eqnarray}\label{equ:EFA-X-common-factor}
    \vX = \vXi\vLambda_c\T + \vDelta
\end{eqnarray}
其中 $ \vXi = [\xi_1, \xi_2, \cdots, \xi_c] $，$ \vDelta = [\delta_1, \delta_2, \cdots, \delta_n] $，
$ \vLambda_c $ 是一个 $ p \times c $ 的系数矩阵。
另外，我们使用下面三个关于公因子模型成分的假设
\begin{enumerate}
    \item 公因子 $ \xi $ 互不相关，且有单位方差 $$ \frac{1}{n-1}\vXi\T\vXi = \vI $$
    \item 特殊因子 $ \delta $ 互不相关，且拥有对角化协方差矩阵
    $$ \vTheta = \frac{1}{n-1}\vDelta\T\vDelta = \diag(\theta_{11}, \theta_{22}, \cdots, \theta_{pp}) $$
    \item 公因子 $ \xi $ 和特殊因子 $ \delta $ 互不相关 $$ \vXi\T\vDelta = \vzero $$
\end{enumerate}

现在我们将公式\eqref{equ:EFA-R-define}中的 $ \vX $ 用公式\eqref{equ:EFA-X-common-factor}代替，
来对相关系数矩阵 $ \vR $ 进行近似：
\begin{align}
    \begin{split}
        \vR & = \frac{1}{n-1}(\vXi\vLambda_c\T + \vDelta)\T(\vXi\vLambda_c\T + \vDelta) \\ 
        & = \frac{1}{n-1}(
            \vLambda_c\vXi\T\vXi\vLambda_c +
            \vDelta\T\vXi\vLambda_c\T +
            \vLambda_c\vXi\T\vDelta +
            \vDelta\T\vDelta
        )
    \end{split}
\end{align}
基于公因子模型的假设 3，上式圆括号中的第二项和第三项为零。
基于假设 1，第一项中的表达式 $ \hfrac{1}{(n-1)}\vXi\T\vXi $ 可以替换为单位矩阵 $ \vI $。
基于假设 2，最后一项的表达式变为 $ \vTheta $。
通过这些简化，我们可以得到
\begin{equation}
    \vR = \vLambda_c\vLambda_c\T + \vTheta
\end{equation}
或者
\begin{equation}\label{equ:EFA-R-efa}
    \vR - \vTheta = \vLambda_c\vLambda_c\T
\end{equation}

将主成分分析模型的公式\eqref{equ:EFA-R-pca}和公因子模型的公式\eqref{equ:EFA-R-efa}进行对比，
可以发现这两个方法之间的相似性以及本质区别。
两种情况下，我们都是对一个二次对称矩阵进行分解。
矩阵 $ \vLambda_c $ 事实上与矩阵 $ \vF_c $ 同构：
是因子载荷矩阵，其元素可被解释为原始变量 $ \vX $ 和提取的 $ c $ 个公因子的相关系数。

\subsubsection{旋转不确定性}

在主成分分析中，我们按顺序方式选择每个成分，以解释原始数据中尽可能大的变异，条件是与所有先前选定的主成分不相关。
这确保了一个唯一解，虽然在选择的方向上有点武断。
但是在公因子模型中，我们没有强加这种约束。
结果是，事实上有无穷多解，他们对矩阵 $ \vR - \vTheta $ 能够近似的程度是相同的。
我们将这个属性称为公因子模型的\mindex{旋转不确定性}{rotational indeterminacy}。

我们首先通过一个例子来展示这种不确定性。
考虑表\ref{tab:EFA-factor-analysis-with-smcs-as-initial-estimates}所示的考试的分数据的双因子解。
图\ref{fig:diagram-showing-clockwise-rotation}展示了由该表中因子载荷矩阵所绘制的图表。
我们现在通过将他们顺时针旋转30°来改变因子的方向。
旋转（通过矩阵乘法进行）保留了两个因子的正交性。
由第\ref{chapter:vectors-and-matrices}章，我们知道进行正交旋转（二维）的矩阵具有以下形式：
\begin{equation}
    \vT = \begin{pmatrix}
        \cos\alpha & -\sin\alpha \\ 
        \sin\alpha & \cos\alpha
    \end{pmatrix}
\end{equation}
当旋转角 $ \alpha = -30 $ 度时，我们得到下列正交旋转矩阵 $ \vT $
\begin{equation}
    \vT = \begin{pmatrix}
        0.866 & 0.500 \\
        -0.500 & 0.866
    \end{pmatrix}
\end{equation}

\begin{figure}\CenterFloatBoxes
    \begin{floatrow}
        \ffigbox[\FBwidth]{
            \begin{tikzpicture}[scale=3.5]
                \draw (-1,0) -- (1,0);
                \draw (0,-1) -- (0,1) coordinate [label=left:未旋转] (Ymax);
                \draw [rotate=60,line width=2pt] (0,-1) -- (0,1);
                \draw [rotate=60,line width=2pt] (-1,0) -- (1,0) coordinate [label=right:旋转] (YmaxR);
                \coordinate (O) at (0,0);
                \coordinate [label={[right]:PARA}]  (PARA) at (0.8349,-0.2418);
                \coordinate [label={[right]:SENT}]  (SENT) at (0.8253,-0.1398);
                \coordinate [label={[below]:WORD}]  (WORD) at (0.7898,-0.2674);
                \coordinate [label={[right]:ADD }]  (ADD ) at (0.4146, 0.6503);
                \coordinate [label={[above,xshift=-1em]:DOTS}]  (DOTS) at (0.3297, 0.6890);
                \draw (O) -- (PARA);
                \draw (O) -- (SENT);
                \draw (O) -- (WORD);
                \draw (O) -- (ADD );
                \draw (O) -- (DOTS);
                \draw [<-,>=stealth] (0,0) + (60:0.95) arc [start angle=60,end angle=90,radius=0.95];
                \draw [<-,>=stealth] (0,0) + (150:0.95) arc [start angle=150,end angle=180,radius=0.95];
            \end{tikzpicture}
        }{\caption{顺时针旋转30°的示意图}\label{fig:diagram-showing-clockwise-rotation}}
        \killfloatstyle
        \ttabbox[\FBwidth]{
            \caption{原始因子载荷和旋转-30°后的因子载荷}
            \label{tab:EFA-initial-factor-loadings-and-factor-loadings-rotated}
        }{
            \begin{tabular}{lrrrrr}\toprule
                & \multicolumn{2}{c}{\textbf{原始载荷}} & & \multicolumn{2}{c}{\textbf{旋转-30°的载荷}} \\\cmidrule{2-3}\cmidrule{5-6}
                     & Factor 1 & Factor 2 & & Factor 1 & Factor 2 \\ 
                PARA & $ 0.811$ & $-0.200$ & & $ 0.801$ & $ 0.232$ \\ 
                SENT & $ 0.811$ & $-0.106$ & & $ 0.754$ & $ 0.313$ \\ 
                WORD & $ 0.779$ & $-0.195$ & & $ 0.773$ & $ 0.221$ \\ 
                ADD  & $ 0.375$ & $ 0.586$ & & $ 0.032$ & $ 0.695$ \\ 
                DOTS & $ 0.295$ & $ 0.614$ & & $-0.052$ & $ 0.679$ \\ 
                \bottomrule
            \end{tabular}
        }
    \end{floatrow}
\end{figure}

通过改变代表公因子的轴的方向，我们也改变了因子载荷的值。
我们可以计算新的载荷（记为 $ \vLambda_c^* $），正好是旋转后的因子 $ \vXi\vT $ 
和原始变量 $ \vX $（由 $ \vXi\vLambda_c\T + \vDelta $ 给出）的相关系数，简化为
\begin{equation}
    \vLambda_c^* = \frac{1}{n-1}(\vXi\vLambda_c\T+\vDelta)\T\vXi\vT
\end{equation}
由于 $ \hfrac{1}{(n-1)}\vXi\T\vXi = \vI $ 且 $ \vDelta\T\vXi = 0 $。
旋转后的因子载荷 $ \vLambda_c^* $ 如表\ref{tab:EFA-initial-factor-loadings-and-factor-loadings-rotated}
中原始未旋转载荷的旁边所示。
正如之前所说的，矩阵中的主要载荷（即那些拥有高绝对值的载荷）并没有发生显著改变：
前三项考试在第一个因子的载荷和后两个在第二个因子的载荷。
但是注意旋转后的解的交叉载荷改变了。
前三项考试现在在第二个因子上由正载荷（而不是负载荷），
而后两项考试则几乎完全负载在第二个因子上（而不是在两个因子上都有正载荷）。

关于旋转因子解有两个重要的性质。
第一，尽管每个因子解释的方差发生了变化，两个因子解释的总方差仍然相同。
因为未旋转的解由分解矩阵 $ \vR - \vTheta $ 获得，
这个解的方向使第一个因子能够解释最多的变异、第二个因子能够解释剩下的变异。
旋转改变了方向，使得由第一个因子解释的变异变小了。
但是旋转仅改变公因子空间轴的方向，所以并不影响解释的总方差。

第二个旋转解的性质使共同度没有改变。通过从旋转的因子载荷重构矩阵 $ \vR - \vTheta $ 可以很容易发现这一性质。
我们有
\begin{align}
    \begin{split}
        \vR - \vTheta & = \vLambda_c^*{\vLambda_c^*}\T \\ 
        & = \vLambda_c\vT(\vLambda_c\vT)\T \\ 
        & = \vLambda_c\vT\vT\T\vLambda_c\T \\ 
    \end{split}
\end{align}
进行矩阵乘法，很容易发现 $ \vT\vT\T = \vI $ ：
\begin{equation}
    \begin{pmatrix}
        \cos^2\alpha + \sin^2\alpha = 1 & -\cos\alpha\sin\alpha + \sin\alpha\cos\alpha = 0 \\ 
        \sin\alpha\cos\alpha - \cos\alpha\sin\alpha = 0 & \cos^2\alpha + \sin^2\alpha = 1 \\ 
    \end{pmatrix}
\end{equation}
更一般地，任何维度的任何正交旋转矩阵都有 $ \vT\vT\T = \vI $ 。
这是因为矩阵转置实际上将轴按照反方向转了回去，得到了最初的方向。

共同度和被公因子解释的变异以及公因子模型的拟合都不受正交旋转的影响，
这一重要结论在后面我们考虑增强因子解的可解释性时会非常有用。

\subsubsection{因子旋转}

为了找到最合适的旋转，我们必须找到一种方法以目标函数的形式定量化表达通过简化结构我们想要什么。
然后我们搜索所有可能的旋转角度，然后选择矩阵 $ \vT $ 使满足旋转后的因子载荷矩阵 $ \vA = \vLambda_c\vT $
对于简化的结构展现出高的目标函数的值。
有许多不同的方法来量化最简结构，我们只讨论两个： Kaiser 的最大方差正交旋转法和四次方最大旋转法。

\paragraph{Kaiser 的最大方差正交旋转法}
回忆旋转后的载荷矩阵每个元素 $ a_{ik} $ 可以被解释为变量 $ i $ 和公因子 $ k $ 间的相关系数。
载荷的平方 $ a_{ik}^2 $ 是变量 $ i $ 中由公因子 $ k $ 引起的变异的比例。
由于我们所选择的公因子互不相关，所有公因子所能解释的变异大小之和——即所谓的共同度——可以由载荷平方和给出，
也就是 $ h_i^2 = \sum_k a_{ik}^2 $ 。
为了实现最简结构，我们想要找到一个旋转使得平方载荷 $ a_{ik}^2 $ 要么接近 1，要么接近 0。

最大方差过程通过关注 $ \vA $ 的列来尝试实现这一目的：它选择旋转矩阵 $ \vT $ 来最大化 $ a_{ik}^2 $ 的列方差之和。
第 $ k $ 个列方差由下式给出
\begin{equation}
    V_k = \frac{1}{p} \sum_{i=1}^{p}\left(a_{ik}^2\right)^2 - \frac{1}{p^2}\left(\sum_{i=1}^{p}a_{ik}^2\right)^2
\end{equation}
对所有因子 $ k $ 最大化列方差 $ V_k $ 之和等同于最大化下式
\begin{equation}
    V = \sum_{k=1}^{c}\sum_{i=1}^{p}a_{ik}^4 - \frac{1}{p}\sum_{k=1}^{c}\left(\sum_{i=1}^{p}a_{ik}^2\right)^2
\end{equation}
注意到当 $ a_{ik}^2 $ 的值趋向于 0 或 1 时可以得到最大方差；
根据定义，对于某些因子 $ k $，当 $ a_{ik}^2 $ 的值趋向于 1，所有其他在矩阵中同一行上的项都趋于 0。 

对于最大方差正交旋转法使用归一化平方载荷（$ \hfrac{a_{ik}^2}{h_i^2} $）建立目标函数也是可能的。
如果使用这种方式归一化载荷，$ \hfrac{a_{ik}^2}{h_i^2} $ 可以被解释为
变量 $ i $ 由于公因子 $ k $ 引起变异大小的比例。
使用这种归一化确保在选择旋转时所有矩阵由相同的权重
（当一些变量有较低共同度时对于没有归一化的平方载荷则不是这种情况）。

\paragraph{四次方最大旋转法}
与最大方差正交旋转法关注旋转的因子载荷矩阵 $ \vA $ 的列不同，四次方最大旋转法关注行。
四次方最大旋转法的目标函数依赖于正交旋转前后变量的共同度不变这一事实；
因此，表达式 $ \sum_k a_{ik}^2 $ 是常数，与旋转矩阵 $ \vT $ 无关。
对于所有变量，共同度的平方和 $ \sum_i(\sum_k a_{ik}^2)^2 $ 也是一个常数。
扩展这个表达式得到
\begin{equation}
    \sum_{i=1}^{p}\sum_{k=1}^{c} a_{ik}^4 + \sum_{i=1}^{p}\left(\sum_{k=1}^{c}\sum_{j\neq k}a_{ik}^2a_{ij}^2\right)
\end{equation}

上式的第二项是平方载荷的交叉积。当矩阵具有简单结构时，该积应当尽可能小
（即当一个变量在因子 $ k $ 上有高载荷，对其他所有因子 $ j $ 载荷应当接近 0）。
因为上式对于所有旋转矩阵 $ \vT $ 是常数，一种保证交叉积项最小的方法是最大化表达式的第一项。
因此，四次方最大旋转法选择一个正交旋转矩阵 $ \vT $ 使
\begin{equation}
    Q = \sum_{i=1}^{p}\sum_{k=1}^{c} a_{ik}^4
\end{equation}
最大。与最大方差正交旋转法相同，在进行旋转之前，可以对平方载荷通过除以变量的共同度进行归一化。

我们在下面的章节中讨论倾斜旋转（旋转不能保持因子的相互正交性）的问题。

\subsubsection{因子得分}

通常，因子分析本身并不是目的，而是进一步分析数据的中间步骤。
对于后续分析，我们需要获得在缩减的因子空间中每个原始观测的位置。
这个值被称为\mindex{因子得分}{factor scores}。

从公因子模型中得到因子得分并不像从主成分分析中得到成分得分一样容易。
回想一下，主成分得分是原始变量的线性组合，可以使用来自相关矩阵的特征向量的系数直接计算。
在公因子分析中，由于特定因素引入的不确定性，得分无法准确计算。
换句话说，在公因子模型中我们不能将 $ \xi $ 在不知道 $ \delta $ 的情况下写成 $ X $ 的函数。

因此，有必要估计我们将在计算因子得分时使用的因子得分系数。
我们用下述线性表达式近似 $ \vXi $ 
\begin{equation}\label{equ:EFA-Xi-approximate}
    \vXi = \vX_s\vB
\end{equation}
其中 $ \vB $ 时因子得分系数矩阵。由于 $ \vXi $ 的值不能直接被观测到，我们不能使用最小二乘回归计算 $ \vB $。
但是，如果我们对上式两边同时乘以 $ \hfrac{1}{(n-1)}\vX_s\T $ 我们可以得到
\begin{equation}
    \frac{1}{n-1}\vX_s\T\vXi = \frac{1}{n-1}\vX_s\T\vX_s\vB
\end{equation}
或
\begin{equation}
    \vLambda_c = \vR\vB
\end{equation}

在保证等式不变的情况下，我们将两边同时乘以 $ \vR^{-1} $，对于因子得分系数有
\begin{equation}
    \vB = \vR^{-1}\vLambda_c
\end{equation}
将这个式子中的 $ \vB $ 代入式\eqref{equ:EFA-Xi-approximate}中得到用于估计因子得分的下式
\begin{equation}\label{equ:EFA-Xi-estimate}
    \vXi = \vX_s \vR^{-1} \vLambda_c
\end{equation}

注意到由于载荷矩阵 $ \vLambda_c $ 满足旋转不确定性，式\eqref{equ:EFA-Xi-estimate}得到的因子得分
并不唯一，并且具有相同的旋转不确定性。但是乘积 $ \hat{\vX}_c = \vXi\vLambda_c\T $ 是不变的，
其中 $ \hat{\vX}_c $ 由 $ c $ 个潜在因子拟合的 $ \vX $ 的一部分。

\section{注意事项}

\subsection{是否可以获得具有相关性的因子解}

当提取因子或组分时（在主成分分析或公因子模型中），我们强制假设正交性（即强加公共因子相互不相关的假设），目的是便于解释。
然而，一旦我们提取了因子，理论上没有理由（尽管下面讨论了一些实际考虑因素）不能选择允许因子相关的后续旋转。
通过不约束旋转为正交，可以更好地近似变换的载荷矩阵中的简单结构，从而提高解的可解释性。
我们将这些非正交旋转称为倾斜旋转。
倾斜旋转背后的意图是使因子轴尽可能与原始数据集中的变量组对齐——或以其他方式促进所得配置的可解释性——无论结果因素是否不相关。
因此，倾斜旋转的目标函数类似于正交旋转的目标函数（例如，最小化平方因子负载的叉积之和），但没有正交性的约束。

通常，倾斜变换导致与使用正交旋转时不关心的因子解的解释相关的若干问题。
主要问题是，通过倾斜旋转，需要区分所谓的结构载荷和模式载荷。
图\ref{fig:EFA-diagram-depicting-difference-between-structure-loadings-and-pattern-loadings}说明了区别。
假设我们在倾斜系统中通过倾斜旋转 $ \vLambda_c $ 得到一些点 $ \vp $ ，
原始（正交）因子解变为了新矩阵 $ \vA^* = \vLambda_c\vT^* $ 其中 $ \vT^* $ 是倾斜协转变换矩阵。
角度 $ \psi $ 反映了因子 $ \va_1^* $ 和 $ \va_2^* $ 在倾斜系统中的协方差。

\begin{figure}[htb]
    \ffigbox[\textwidth]{
        \centering
        \begin{tikzpicture}
            \coordinate (O) at (0,0);
            \coordinate [label=right:$a_1^*$] (a1) at ($ (0:4) $);
            \coordinate [label=right:$a_2^*$] (a2) at ($ (40:4) $);
            \coordinate [label=right:$x$] (X) at ($ (0:3) + (0,{1*sin(40)}) $);
            \coordinate [label=below:$x_1$] (X1) at ($ (a1)!(X)!(O) $);
            \coordinate [label=left:$x_2$] (X2) at ($ (a2)!(X)!(O) $);
            \coordinate [label=left:$w_2$] (W2) at ($ (40:1) $);
            \coordinate [label=below:$w_1$] (W1) at ($ (0:{3-1*cos(40)}) $);
            \draw (a1) -- (O) -- (a2);
            \draw (X) -- (X1);
            \draw (X) -- (X2);
            \draw (X) -- (W2);
            \draw (X) -- (W1);
            \draw (X1) rectangle ++(0.1,0.1);
            \draw [rotate=-50] (X2) rectangle ++(0.1,0.1);
            \draw ($ (O) + (0.4,0) $) arc [start angle=0,end angle=40,radius=0.4] (20:0.6) node {$ \psi $};
            \coordinate [label={[below,xshift=-1.5em]:模式载荷}] (pattern) at ($ (W1) - (0.1,0.6) $);
            \coordinate [label={[below,xshift=1.5em]:结构载荷}] (structure) at ($ (X1) - (-0.1,0.6) $);
            \draw [->,>=stealth] (pattern) -- ($ (W1) - (0,0.35) $);
            \draw [->,>=stealth] (structure) -- ($ (X1) - (0,0.35) $);
        \end{tikzpicture}
    }{
        \caption{结构载荷和模式载荷区别的示意图}
        \label{fig:EFA-diagram-depicting-difference-between-structure-loadings-and-pattern-loadings}
    }
\end{figure}

注意到我们有两种方法可以描述点 $ \vp $ 在倾斜因子空间的位置。
一种方法是，我们可以描述 $ \vp $ 在轴 $ \va_1^* $ 和 $ \va_2^* $ 上的投影（正交），如图中 $ x_1 $ 和 $ x_2 $ 所示。
在因子分析术语中，这被称为\mindex{结构载荷}{structure loadings}。
另一种方法，我们可以描述 $ \vp $ 在轴 $ \va_1^* $ 和 $ \va_2^* $ 上的坐标，如图中 $ w_1 $ 和 $ w_2 $ 所示。
这被称为\mindex{模式载荷}{pattern loadings}。
当因子正交时，结构载荷和模式载荷之间没有区别。

结构载荷是变量和因子之间的相关系数（因此范围是 $ -1 $ 到 $ 1 $）。
这些是由变换后的载荷矩阵 $ \vA^* = \vLambda_c\vT^* $ 给出的值。
通常，结构载荷在解释因子时一般没什么帮助，因为简单结构被潜在因自相关性掩盖掉了。
注意，在图中，由于两个因子的相关性，$ x_1 $ 和 $ x_2 $ 都相对较大。
相反，模式载荷类似于偏回归系数，因为它们考虑了其他因素所导致的变化。
换句话说，他们是倾斜因子线性组合的系数，这允许我们重建原始配置。
结果是，模式载荷没有在 $ -1 $ 到 $ 1 $ 之间变化的约束，但是在解释因子解的过程中更有用。
如图中所示， $ w_1 $ 比 $ w_2 $ 大得多，暗示着点 $ \vp $ 与第一个因子更接近。
从下式中可以获得模式载荷
\begin{equation}
    \vA^*\vP
\end{equation}
其中 $ \vP $ 是倾斜坐标系统中具有相关性的因子之间夹角的方向余弦（适当标准化）组成的矩阵。
在图\ref{fig:EFA-diagram-depicting-difference-between-structure-loadings-and-pattern-loadings}中所示二维示例中，
模式载荷和结构载荷的关系由下式给出
\begin{equation}
    \begin{pmatrix}
        w_1 & w_2
    \end{pmatrix} = \begin{pmatrix}
        x_1 & x_2
    \end{pmatrix} \begin{pmatrix}
        1 & \cos\psi \\
        \cos\psi & 1
    \end{pmatrix}
\end{equation}

在处理倾斜解时有另一个警告：由于因子可能相关，变量的共同度不能通过求位于因子载荷矩阵第 $ i $ 行的载荷的平方和进行计算，
而且因子解释的变异也不能通过求位于因子载荷矩阵第 $ k $ 列的载荷的平方和进行计算。

我们现在通过回到关于心理测试的说明性示例来说明倾斜旋转。
检查未旋转的因子载荷矩阵（如表\ref{tab:EFA-factor-analysis-with-smcs-as-initial-estimates}所示），
对双因子解的解释仍然很清晰：
前三项考试主要在第一个因子上具有负载（我们应当解释为文学天赋），
后两项主要在第二个因子上具有负载（我们应当解释为计量天赋）。
问题是，如果我们对这个解应用了某种倾斜旋转，我们能否得到一个更加清晰地简单结构的图景？

为了回答这个问题，我们会使用一种被称作目标矩阵拟合的方法进行倾斜变换。
目标矩阵代表了一个特别的问题中潜在因子简单结构的表示。
它可能由基本理论构成，或代表被解释的因子载荷矩阵的结构。
在这个特别的例子中，我们的目标矩阵是一个简单的矩阵，为
\begin{equation}
    \vG = \begin{pmatrix}
        1 & 0 \\ 
        1 & 0 \\ 
        1 & 0 \\ 
        0 & 1 \\ 
        0 & 1 \\ 
    \end{pmatrix}
\end{equation}

我们的目标是找到一个变换矩阵，将因子载荷矩阵 $ \vLambda_c $ 旋转到与目标矩阵 $ G $ 最接近一致的可能。
如果我们使用 $ \vT^* $ 表示变换矩阵，那么我们的目标是选择 $ \vT^* $
使最终变换 $ \vLambda_c\vT^* $ 与目标矩阵 $ \vG $ 尽可能一致。
表示拟合到 $G$ 的最佳最小二乘的变换由下式给出
\begin{equation}
    \vT^* = (\vLambda_c\T\vLambda_c)^{-1} \vLambda_c\T\vG
\end{equation}

这个过程和将 $ \vG $ 的每一列用 $ \vLambda_c $ 进行回归然后再对 $ \vT^* $ 的结果列进行标准化没有什么区别。
注意到 $ \vT^* $ 的列通常情况下不正交。
这种变换有时被称为\mindex{普洛克拉斯提恩旋转}{Procrustean rotation}（参见 Hurley and Cattell, 1962）。

心理学考试数据双因子解的倾斜旋转结果如表\ref{tab:EFA-results-from-oblique-rotation-of-psychological-test-data}所示。
模式载荷展示了目标简单结构非常接近的近似：因子模式中的交叉载荷的绝对值在 $ 0.02 $ 到 $ 0.06 $ 附近。
因此，我们有了一个简单解，第一个因子由前三项考试度量，第二个因子由后三项考试度量。

\begin{table}[t]
    \ttabbox{
        \caption{心理学测试数据倾斜旋转结果}
        \label{tab:EFA-results-from-oblique-rotation-of-psychological-test-data}
    }{
        \begin{tabular}{lrrcrrc|crc}\toprule
            & \multicolumn{2}{c}{\textbf{模式载荷}} & 
            & \multicolumn{2}{c}{\textbf{结构载荷}} & &
            & \multicolumn{2}{c}{\textbf{因子间相关系数}} \\
            \cline{2-3}\cline{5-6}\cline{9-10}
            & 因子 1 & 因子 2 & & 因子 1 & 因子 2 & & & & 因子 2\\ 
            $ PARA $ & $ 0.8480$ & $-0.0300$ & & $ 0.8687$ & $ 0.1906$ & & & 因子 1 & $0.2528$ \\
            $ SENT $ & $ 0.8480$ & $-0.0300$ & & $ 0.8687$ & $ 0.1906$ & & & & \\
            $ WORD $ & $ 0.8480$ & $-0.0300$ & & $ 0.8687$ & $ 0.1906$ & & & & \\
            $ ADD  $ & $ 0.8480$ & $-0.0300$ & & $ 0.8687$ & $ 0.1906$ & & & & \\
            $ DOTS $ & $ 0.8480$ & $-0.0300$ & & $ 0.8687$ & $ 0.1906$ & & & & \\
            \bottomrule
        \end{tabular}
    }
\end{table}

倾斜变换导致了两个因子之间的相关系数为 0.25。
这个微弱的相关表示具有高语言天赋（至少是考试中测量出来的篇章理解、句子完成、单词含义的能力）
也会拥有比平均水平高的计量天赋，反之相同。
注意到这个因子间的相关性反映在结构相关系数：所有矩阵中的交叉载荷都大约是 0.25 。
因此，我们牺牲了正交旋转的一些简单性，换来一种可以说是“更清洁”的因素解决方案。
一般地，将正交解和倾斜解都考虑一下来看哪个可以更好地服务目标并不是一个坏主意。

一个 RTE 谷物数据地倾斜解既不能提供一个不同的解释也不能提供简单结构实质上更清晰地图景。
在倾斜空间中，所有四个因子互相具有正相关（可能是一些潜在的光环效应的反映），
只有一个例外：因子 1 和因子 2 有较小的负相关。
这与认为谷物是健康的（即在因子 1 上有高值）更可能被视为非“人造”的（因子 2 上有低值）的观念是一致的。


\subsection{如何在后续分析中使用这些结果}

通常，因子分析本身并不是目的，而是进一步分析数据的中间步骤。
例如，Roberts 和 Lattin 使用因子分析来确定可用于构建RTE谷物考虑模型的较少数量的潜在维度。
对于这种类型的后续分析，我们需要知道每个原始观测在缩减因子空间中的位置。
这些值称为\mindex{因子得分}{factor scores}。

表\ref{tab:factor-score-coefficients-for-rte-cereal-data}给出了 RTE 谷物数据的四因子最大方差正交旋转解的因子得分系数。
我们使用这些系数计算数据集中 235 个观测值中的每一个的因子得分，然后计算 12 个评级品牌中的每一个的平均因子得分。
这些平均分数（如表\ref{tab:average-factor-scores-for-rte-cereals}所示）
绘制在图\ref{fig:EFA-scatter-plots-of-factor-scores-for-rte-cereals}中；
为了节省空间，我们刚刚展示了两个成对图（因子 1 对比因子 2 和因子 3 对比因子 4）。

\begin{table}[htb]
    \footnotesize
    \begin{floatrow}
        \ttabbox[\FBwidth]{
            \caption{RTE 谷物数据的因子得分系数}
            \label{tab:factor-score-coefficients-for-rte-cereal-data}
        }{
            \begin{tabular}{lrrrr}\toprule
                & \multicolumn{4}{c}{\bfseries 因子得分系数} \\\cmidrule{2-5}
                & 因子 1 & 因子 2 & 因子 3 & 因子 4 \\
                Filling    & $ 0.1456$ & $ 0.0915$ & $ 0.0228$ & $-0.0420$ \\
                Natural    & $ 0.1370$ & $-0.0142$ & $-0.0112$ & $-0.0417$ \\
                Fibre      & $ 0.2160$ & $ 0.0542$ & $-0.1280$ & $-0.0746$ \\
                Sweet      & $ 0.0425$ & $ 0.2261$ & $-0.0030$ & $ 0.0745$ \\
                Easy       & $ 0.0088$ & $ 0.0176$ & $ 0.0663$ & $-0.0115$ \\
                Salt       & $ 0.0301$ & $ 0.2010$ & $ 0.0158$ & $-0.1196$ \\
                Satisfying & $ 0.1101$ & $ 0.0731$ & $ 0.1540$ & $-0.0361$ \\
                Energy     & $ 0.0918$ & $ 0.0752$ & $ 0.0266$ & $ 0.0126$ \\
                Fun        & $-0.0024$ & $ 0.0017$ & $ 0.1017$ & $ 0.1316$ \\
                Kids       & $-0.0433$ & $ 0.0151$ & $ 0.3735$ & $-0.0789$ \\
                Soggy      & $ 0.0390$ & $ 0.0646$ & $ 0.0402$ & $-0.1995$ \\
                Economical & $-0.0043$ & $-0.0453$ & $ 0.1190$ & $-0.0855$ \\
                Health     & $ 0.2155$ & $-0.0792$ & $-0.0091$ & $-0.0590$ \\
                Family     & $-0.0294$ & $-0.0453$ & $ 0.2700$ & $-0.0043$ \\
                Calories   & $ 0.0259$ & $ 0.1567$ & $-0.0080$ & $-0.0253$ \\
                Plain      & $ 0.0319$ & $ 0.0699$ & $ 0.0740$ & $-0.2534$ \\
                Crisp      & $-0.0372$ & $-0.0136$ & $ 0.0864$ & $ 0.1507$ \\
                Regular    & $ 0.0672$ & $-0.0267$ & $-0.0316$ & $ 0.0129$ \\
                Sugar      & $ 0.0444$ & $ 0.3824$ & $-0.0310$ & $-0.0384$ \\
                Fruit      & $ 0.0376$ & $ 0.0321$ & $-0.1393$ & $ 0.1689$ \\
                Process    & $ 0.0091$ & $ 0.1020$ & $ 0.0196$ & $-0.0824$ \\
                Quality    & $ 0.0576$ & $-0.0674$ & $ 0.0531$ & $ 0.0527$ \\
                Treat      & $-0.0279$ & $ 0.0329$ & $ 0.0654$ & $ 0.2471$ \\
                Boring     & $ 0.0327$ & $ 0.0680$ & $-0.0178$ & $-0.1752$ \\
                Nutritious & $ 0.2004$ & $ 0.0045$ & $-0.0229$ & $-0.0711$ \\
                \bottomrule
            \end{tabular}
        }
        \vbox to 1cm {
            \ttabbox[\FBwidth]{
                \caption{12 种 RTE 谷物的平均因子得分}
                \label{tab:average-factor-scores-for-rte-cereals}
            }{
                \begin{tabular}{lcrrrr}\toprule
                    品牌         & 观测数 & 因子 1  & 因子 2  & 因子 3  & 因子 4  \\\midrule
                    All Bran     & $15$     & $0.3490 $ & $-0.3185$ & $-0.8861$ & $-0.3754$ \\
                    Cerola       & $13$     & $0.5174 $ & $0.4994 $ & $-0.2332$ & $0.6582 $ \\
                    Corn flakes  & $27$     & $-0.5541$ & $0.1165 $ & $0.5738 $ & $0.0241 $ \\
                    Just Right   & $16$     & $-0.0133$ & $0.2278 $ & $-0.4196$ & $0.4753 $ \\
                    Komplete     & $14$     & $0.5464 $ & $0.2338 $ & $-1.0084$ & $0.6024 $ \\
                    NutriGrain   & $24$     & $-0.4255$ & $0.8086 $ & $0.5482 $ & $0.2852 $ \\
                    Purina       & $18$     & $0.6226 $ & $0.6838 $ & $-0.4095$ & $0.5754 $ \\
                    Rice Bubbles & $21$     & $-1.1650$ & $-0.4295$ & $0.6041 $ & $0.0627 $ \\
                    Special K    & $23$     & $-0.3058$ & $-0.2142$ & $0.1541 $ & $-0.0549$ \\
                    Sustain      & $12$     & $0.6772 $ & $-0.3021$ & $-0.3234$ & $0.8566 $ \\
                    Vitabrits    & $25$     & $0.3649 $ & $-0.5921$ & $0.2180 $ & $-0.9316$ \\
                    Weetbix      & $27$     & $0.3266 $ & $-0.4120$ & $-0.0713$ & $-0.8837$ \\
                    \bottomrule
                \end{tabular}
            }
            \vspace{1em}
            \killfloatstyle
            \ffigbox{
                \scriptsize
                \floatsetup[figure]{floatrowsep=none}\killfloatstyle 
                \begin{subfloatrow}
                    \ffigbox[0.24\textwidth]{
                        \begin{tikzpicture}[scale=1.5]
                            \def\cross#1{
                                \draw ($(#1)+(-0.03,0)$)--($(#1)+(0.03,0)$);
                                \draw ($(#1)+(0,-0.03)$)--($(#1)+(0,0.03)$);
                            }
                            \foreach \x in {-1.0,-0.5,0.0,0.5} {
                                \draw (\x,-1) -- ++(0,-0.05) node [rotate=0,anchor=north] {$\x$};
                            };
                            \foreach \y in {-0.5,0,0.5} {
                                \draw (-1.3,\y) -- ++(-0.05,0) node [rotate=90,anchor=south] {$\y$};
                            };
                            \node (xlabel) [rotate=0] at (0,-1.3) {Healthful};
                            \node (ylabel) [rotate=90] at (-1.6,0) {Artificial};
                            \draw (-1.3,-1) -- (-1.3,1.2) -- (1,1.2) -- (1,-1) -- cycle;
                            \coordinate [label=left:AllBran]      (AllBran)     at ( 0.3490,-0.3185);
                            \coordinate [label=below:Cerola]      (Cerola)      at ( 0.5174, 0.4994);
                            \coordinate [label=left:Cornflakes]   (Cornflakes)  at (-0.5541, 0.1165);
                            \coordinate [label=above:JustRight]   (JustRight)   at (-0.0133, 0.2278);
                            \coordinate [label=below:Komplete]    (Komplete)    at ( 0.5464, 0.2338);
                            \coordinate [label=below:NutriGrain]  (NutriGrain)  at (-0.4255, 0.8086);
                            \coordinate [label=above:Purina]      (Purina)      at ( 0.6226, 0.6838);
                            \coordinate [label=right:RiceBubbles] (RiceBubbles) at (-1.1650,-0.4295);
                            \coordinate [label=left:SpecialK]     (SpecialK)    at (-0.3058,-0.2142);
                            \coordinate [label=above:Sustain]     (Sustain)     at ( 0.6772,-0.3021);
                            \coordinate [label=below:Vitabrits]   (Vitabrits)   at ( 0.3649,-0.5921);
                            \coordinate [label=right:Weetbix]     (Weetbix)     at ( 0.3266,-0.4120);
                            \cross{AllBran};
                            \cross{Cerola};
                            \cross{Cornflakes};
                            \cross{JustRight};
                            \cross{Komplete};
                            \cross{NutriGrain};
                            \cross{Purina};
                            \cross{RiceBubbles};
                            \cross{SpecialK};
                            \cross{Sustain};
                            \cross{Vitabrits};
                            \cross{Weetbix};
                        \end{tikzpicture}
                    }{\caption{}}
                    \ffigbox[0.24\textwidth]{
                        \begin{tikzpicture}[scale=1.5]
                            \def\cross#1{
                                \draw ($(#1)+(-0.03,0)$)--($(#1)+(0.03,0)$);
                                \draw ($(#1)+(0,-0.03)$)--($(#1)+(0,0.03)$);
                            }
                            \foreach \x in {-1.0,-0.5,0.0,0.5} {
                                \draw (\x,-1) -- ++(0,-0.05) node [rotate=0,anchor=north] {$\x$};
                            };
                            \foreach \y in {-0.5,0,0.5} {
                                \draw (-1.3,\y) -- ++(-0.05,0) node [rotate=90,anchor=south] {$\y$};
                            };
                            \node (xlabel) [rotate=0] at (0,-1.3) {Nonadult};
                            \node (ylabel) [rotate=90] at (-1.6,0) {Interesting};
                            \draw (-1.3,-1) -- (-1.3,1.2) -- (1,1.2) -- (1,-1) -- cycle;
                            \coordinate [label=right:AllBran]      (AllBran)     at (-0.8861,-0.3754);
                            \coordinate [label=right:Cerola]      (Cerola)      at (-0.2332, 0.6582);
                            \coordinate [label=below:Cornflakes]  (Cornflakes)  at ( 0.5738, 0.0241);
                            \coordinate [label=below:JustRight]   (JustRight)   at (-0.4196, 0.4753);
                            \coordinate [label=above:Komplete]    (Komplete)    at (-1.0084, 0.6024);
                            \coordinate [label=above:NutriGrain]  (NutriGrain)  at ( 0.5482, 0.2852);
                            \coordinate [label=left:Purina]      (Purina)      at (-0.4095, 0.5754);
                            \coordinate [label=above:RiceBubbles] (RiceBubbles) at ( 0.6041, 0.0627);
                            \coordinate [label=left:SpecialK]    (SpecialK)    at ( 0.1541,-0.0549);
                            \coordinate [label=above:Sustain]     (Sustain)     at (-0.3234, 0.8566);
                            \coordinate [label=right:Vitabrits]   (Vitabrits)   at ( 0.2180,-0.9316);
                            \coordinate [label=left:Weetbix]     (Weetbix)     at (-0.0713,-0.8837);
                            \cross{AllBran};
                            \cross{Cerola};
                            \cross{Cornflakes};
                            \cross{JustRight};
                            \cross{Komplete};
                            \cross{NutriGrain};
                            \cross{Purina};
                            \cross{RiceBubbles};
                            \cross{SpecialK};
                            \cross{Sustain};
                            \cross{Vitabrits};
                            \cross{Weetbix};
                        \end{tikzpicture}
                    }{\caption{}}
                \end{subfloatrow}
            }{
                \vspace{-1em}
                \caption{RTE 谷物因子得分散点图}
                \label{fig:EFA-scatter-plots-of-factor-scores-for-rte-cereals}
            }
        }
    \end{floatrow}
\end{table}

平均因子得分与我们之前关于这些谷物相对位置的概念一致。
例如，Weetbix 和 Vitabrits（每种都是切碎的小麦型饼干）都有类似的感觉：健康，非人工和枯燥。
三种不同品牌的牛奶什锦早餐（Cerola、Komplete 和 Purina）也在因子空间中聚集在一起。

\subsection{如何评估因子结构的有效性}

通过所有探索性程序，我们需要评估我们的描述性分析在多大程度上捕获整个人口的特征，而不仅仅是我们所选样本的特征。
因此，我们需要提出这个问题：分析的结果是否可以推广？ 
因为因子分析的一个重要目标是对潜在因素结构的解释，
我们可能会问，如果我从同一群体中选择了不同的样本，我是否会得出基本相同的解释？

回答这个问题的一种方法是使用一个保留样本。
如果我们的横截面样本的大小足够大，那么我们可以简单地将数据分成两个大小相等的独立组（使用随机分配来确保可比性）。
然后，我们可以对每个因子进行因子分析，并比较因子加载矩阵的解释相似性。
然而，基于偶然视觉检查的比较很少令人满意。
可能涉及数百个数字，使得比较困难。
此外，该任务是主观的并且受到一厢情愿的思考（即，研究人员可能倾向于抓住矩阵之间的相似性并淡化对比度）。
更严格的方法涉及使用验证性因子分析来使用来自保持样本的数据来测试第一样本中鉴定的特定因子结构。
如果未拒绝指定的因子结构（即，模型提供对保持数据的合理拟合），那么我们可以得出结论，解释超出了第一个数据样本。
第\ref{chapter:ConfirmatoryFactorAnalysis}章介绍并讨论了验证性因子分析。

另一种方法涉及评估采样变化对因子结构的影响（通过使用保持样本或自举采样）。
让我们假设我们使用以下过程来识别具有 $ c $ 个共同因子的解决方案的基础结构。
在某种类型的旋转之后（假设为最大方差正交旋转），我们确定因子载荷矩阵的每一行中的最高载荷（绝对值）并将该变量与该因子相关联。
然后，我们基于与其相关的变量（即，高正负载或高负负载）来解释每个公共因子。
实际上，这相当于用 0 和 1 代替因子载荷矩阵，表示因子解决方案的简单结构。
结果是一个每个变量与一个且只有一个公共因子相关联的划分。
因子载荷矩阵的这种“结构化”构成了我们解释的基础。我们使用 $ \vS_0 $ 来表示原始数据的因子加载矩阵的这种简单结构变换。
RTE 谷物数据的矩阵 $ \vS_0 $ 如表5.15所示。
请注意，已重新排序属性，以便将加载在同一因子上的属性一起列出。

\begin{table}[t]
    \centering
    \ttabbox{
        \caption{最大因子载荷的零一矩阵}
        \label{tab:EFA-zeroone-matrix-of-maximum-factor-loadings}
    }{
        \begin{tabularx}{\textwidth}{XYYYY}\toprule
            & 因子 1 & 因子 2 & 因子 3 & 因子 4 \\\midrule
            Filling    & 1 & 0 & 0 & 0 \\
            Natural    & 1 & 0 & 0 & 0 \\
            Fibre      & 1 & 0 & 0 & 0 \\
            Satisfying & 1 & 0 & 0 & 0 \\
            Energy     & 1 & 0 & 0 & 0 \\
            Health     & l & 0 & 0 & 0 \\
            Regular    & l & 0 & 0 & 0 \\
            Quality    & l & 0 & 0 & 0 \\
            Nutritious & 1 & 0 & 0 & 0 \\\midrule
            Sweet      & 0 & 1 & 0 & 0 \\
            Salt       & 0 & l & 0 & 0 \\
            Calories   & 0 & l & 0 & 0 \\
            Sugar      & 0 & l & 0 & 0 \\
            Process    & 0 & 1 & 0 & 0 \\\midrule
            Easy       & 0 & 0 & l & 0 \\
            Kids       & 0 & 0 & 1 & 0 \\
            Economical & 0 & 0 & 1 & 0 \\
            Family     & 0 & 0 & l & 0 \\\midrule
            Fun        & 0 & 0 & 0 & 1 \\
            Soggy      & 0 & 0 & 0 & l \\
            Plain      & 0 & 0 & 0 & 1 \\
            Crisp      & 0 & 0 & 0 & 1 \\
            Fruit      & 0 & 0 & 0 & 1 \\
            Treat      & 0 & 0 & 0 & l \\
            Boring     & 0 & 0 & 0 & 1 \\
            \bottomrule
        \end{tabularx}
    }
\end{table}

我们现在问，对于相同人口的不同样本，我们会做出同样的解释吗？
评估这种情况的一种方法是问，如果我们按照上面段落中描述的程序，用不同的样本是否会找到与公因子相关的同一组变量？
对于保留样本，我们可以通过对保持数据进行因子分析，提取 $ c $ 因子（与原始数据上的因子解决方案相同的数量），
并通过上文所述方法创建简单的结构因子加载矩阵（表示为 $ \vS_1 $）来实现此目的。

我们现在有两个划分，$ \vS_0 $ 和 $ \vS_1 $ ，每个都代表了变量到潜在因子的分配。
比较它们的一种方法是创建一个 $ c \times c $ 的交叉表，第 $ (i,j) $ 单元格代表了
在原始数据的分析中与因子 $ i $ 相关而在保留数据分析中与因子 $ j $ 相关的变量数目。
使用矩阵表示法，该交叉表由矩阵乘积 $ S_0\T S_1 $ 给出。
如果原始因子分析的解释是有效的，那么它应该延伸到保留样本。
因此，我们应该期望在分区 $ S_0 $ 和 $ S_1 $ 之间看到强烈的对应关系，
在交叉表的对角线上的元素应较大，在对角线上的元素应较小或为零。
有一些措施，例如 Hubert 和 Arabie（1985）用于比较分区的改进的 Rand 统计量，可以用来量化两个因子结构之间的对应强度。

还可以通过使用自举采样来评估因子结构的特定解释的稳定性。
自举样本只是一个大小为 $n$ 的样本，用原始 $n$ 个观察值替换。
请注意，某些观察结果不会包含在自举样本中，而其他观察结果可能会包含两到三次。
自举样本让我们对假设原始数据集代表基础人口时我们期望的抽样变异的影响有了一些了解。
对于每个自举样本 $k$，我们执行因子分析并形成简单的结构矩阵 $\vS_k$。
然后，我们通过查看所有自举样本 $k$ 中结构矩阵 $\vS_0$ 和 $\vS_k$ 之间的对应关系的变化来评估我们的解释的稳定性。

要评估多个划分的可变性，对模型中所有的变量建立 $ p \times p $ 为交叉表比较容易，
其中第 $ (i,j) $ 单元格表示变量 $ i $ 和 $ j $ 是否关联到同一个因子。
用矩阵的写法，这个交叉表有矩阵乘积 $ \vS_0\vS_k\T $ 给出。
我们可以非常多次（记为 $ K $ 次）自举采样，然后求和 $ \sum_{k=1}^{K}\vS_0\vS_k\T $ 。
生成的交叉表包含使用相同的基础公因子标识每对变量的次数。
如果我们的解释是稳定的（即相对不受我们数据中的采样变化影响的解释），
那么我们应该期望看到在我们的原始数据中负载在一起的变量在大多数或全部自举样本中与相同的因子相关联。

为了说明的目的，我们使用这个程序来评估我们解释 RTE 谷物数据因子解决方案的简单结构的稳定性
（见表\ref{tab:EFA-zeroone-matrix-of-maximum-factor-loadings}）。
我们总共进行了 $ K = 25 $ 个自举样本。
由矩阵交叉积 $ \sum_k\vS_0\vS_k\T $ 之和给出的交叉列表如表\ref{tab:EFA-cross-tabulation-of-25-bootstrap-samples}所示。
表中的模式揭示了有关因子解决方案的几个方面，无法通过仅查看原始数据中的因子载荷矩阵来评估。
首先，我们注意到因素 1 和 2（“健康”和“人工”）的结构非常稳定，至少比基础因素 3 和 4（“非成人”和“有趣”）的结构更为稳定。
在与原始解决方案中的前两个因素相关联的 14 个属性中，
除了两个之外的所有因素在所有 25 个自举样本中表现出与原始样本中完全相同的加载模式。
其余两个属性（满意和已处理）与超过 80\% 的自举样本中的原始结构一致（分别为 25 和 22 中的 21 和 21）。

\begin{table}[htb]
    \small
    \setlength{\tabcolsep}{4pt}
    \ttabbox{
        \caption{25 个自举样本的交叉表}
        \label{tab:EFA-cross-tabulation-of-25-bootstrap-samples}
    }{
        \begin{tabular}{lrrrrrrrrr|rrrrr|rrrr|rrrrrrr}\toprule
            Filling    & 25 & 25 & 25 & 22 & 25 & 25 & 25 & 25 & 25 & 0  & 0  & 0  & 0  & 2  & 3  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 7  & 0  & 0  \\
            Nature     & 25 & 25 & 25 & 22 & 25 & 25 & 25 & 25 & 25 & 0  & 0  & 0  & 0  & 2  & 3  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 7  & 0  & 0  \\
            Fibre      & 25 & 25 & 25 & 22 & 25 & 25 & 25 & 25 & 25 & 0  & 0  & 0  & 0  & 2  & 3  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 7  & 0  & 0  \\
            Satisfying & 22 & 22 & 22 & 25 & 22 & 22 & 22 & 22 & 22 & 0  & 0  & 0  & 0  & 2  & 6  & 3  & 2  & 3  & 2  & 0  & 0  & 1  & 5  & 0  & 0  \\
            Energy     & 25 & 25 & 25 & 22 & 25 & 25 & 25 & 25 & 25 & 0  & 0  & 0  & 0  & 2  & 3  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 7  & 0  & 0  \\
            Health     & 25 & 25 & 25 & 22 & 25 & 25 & 25 & 25 & 25 & 0  & 0  & 0  & 0  & 2  & 3  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 7  & 0  & 0  \\
            Regular    & 25 & 25 & 25 & 22 & 25 & 25 & 25 & 25 & 25 & 0  & 0  & 0  & 0  & 2  & 3  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 7  & 0  & 0  \\
            Quality    & 25 & 25 & 25 & 22 & 25 & 25 & 25 & 25 & 25 & 0  & 0  & 0  & 0  & 2  & 3  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 7  & 0  & 0  \\
            Nutritious & 25 & 25 & 25 & 22 & 25 & 25 & 25 & 25 & 25 & 0  & 0  & 0  & 0  & 2  & 3  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 7  & 0  & 0  \\\hline
            Sweet      & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 25 & 25 & 25 & 25 & 21 & 0  & 0  & 5  & 0  & 0  & 0  & 0  & 0  & 1  & 1  & 0  \\
            Salt       & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 25 & 25 & 25 & 25 & 21 & 0  & 0  & 5  & 0  & 0  & 0  & 0  & 0  & 1  & 1  & 0  \\
            Calories   & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 25 & 25 & 25 & 25 & 21 & 0  & 0  & 5  & 0  & 0  & 0  & 0  & 0  & 1  & 1  & 0  \\
            Sugar      & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 25 & 25 & 25 & 25 & 21 & 0  & 0  & 5  & 0  & 0  & 0  & 0  & 0  & 1  & 1  & 0  \\
            Process    & 2  & 2  & 2  & 2  & 2  & 2  & 2  & 2  & 2  & 21 & 21 & 21 & 21 & 25 & 1  & 0  & 4  & 0  & 0  & 2  & 2  & 1  & 3  & 0  & 1  \\\hline
            Easy       & 3  & 3  & 3  & 6  & 3  & 3  & 3  & 3  & 3  & 0  & 0  & 0  & 0  & 1  & 25 & 21 & 14 & 21 & 14 & 1  & 1  & 13 & 2  & 4  & 4  \\
            Kids       & 0  & 0  & 0  & 3  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 21 & 25 & 18 & 25 & 14 & 0  & 0  & 12 & 3  & 3  & 3  \\
            Economical & 0  & 0  & 0  & 2  & 0  & 0  & 0  & 0  & 0  & 5  & 5  & 5  & 5  & 4  & 14 & 18 & 25 & 18 & 8  & 2  & 2  & 9  & 6  & 2  & 1  \\
            Family     & 0  & 0  & 0  & 3  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 21 & 25 & 18 & 25 & 14 & 0  & 0  & 12 & 3  & 3  & 3  \\\hline
            Fun        & 0  & 0  & 0  & 2  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 14 & 14 & 8  & 14 & 25 & 11 & 11 & 19 & 6  & 14 & 14 \\
            Soggy      & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 2  & 1  & 0  & 2  & 0  & 11 & 25 & 25 & 13 & 14 & 21 & 22 \\
            Palin      & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 2  & 1  & 0  & 2  & 0  & 11 & 25 & 25 & 13 & 14 & 21 & 22 \\
            Crisp      & 0  & 0  & 0  & 1  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 1  & 13 & 12 & 9  & 12 & 19 & 13 & 13 & 25 & 5  & 15 & 16 \\
            Fruit      & 7  & 7  & 7  & 5  & 7  & 7  & 7  & 7  & 7  & 1  & 1  & 1  & 1  & 3  & 2  & 3  & 6  & 3  & 6  & 14 & 14 & 5  & 25 & 11 & 11 \\
            Treat      & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 1  & 1  & 1  & 1  & 0  & 4  & 3  & 2  & 3  & 14 & 21 & 21 & 15 & 11 & 25 & 24 \\
            Boring     & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 0  & 1  & 4  & 3  & 1  & 3  & 14 & 22 & 22 & 16 & 11 & 24 & 25 \\
            \bottomrule
        \end{tabular}
    }
\end{table}

第三和第四个共同因素的结构似乎有更多的变化。
例如，与原始样本中的第四个因子（“有趣”）相关联的属性Crisp和Fun与属于孩子和家庭（来自第三个共同因子“非成人”）的属性相关联，
超过一半 自举样本。 此外，属性“Fruit”加载第四个因子的变量只有大约一半的时间；
其余的时间主要是在第一个因子上加载（偶尔在第二个或第三个因子上加载）。

我们没有统计数据适用于表 \ref{tab:EFA-cross-tabulation-of-25-bootstrap-samples} 中的数字来测试所选因子结构的可靠性。
与探索性因素分析中的许多其他步骤（例如，选择潜在因素的数量，解释解决方案）一样，
因子解释的有效性评估是一种主观判断（在某种程度上甚至完全基于实践）。
即使我们的实质性解释没有改变，分析也是有用的，因为它揭示了结构随样本变化而变化的程度。

在上面的一个验证练习之后，有时候很有诱惑力来“微调”因子解决方案。
这通常采取丢弃变量的形式，这些变量在因子解决方案中的位置似乎不太稳定，以便得出更清晰可解释的因子解决方案。
因此，在RTE谷物数据的背景下，人们可能倾向于放弃变量 Fun、Crisp 和 Fruit 可能并重新运行因子分析，
希望最终解决方案避免上述解决方案中揭示的一些潜在的歧义。

这是对这个诱惑的一个最好的抵抗，至少有两个原因。
首先，变量的选择性保留仍然基于样本中的方差模式，其由可归因于基础种群的变异加上与该特定样本相关的方差组成。
微调可以改善因子解决方案的样本内适应性，但它会危害其超出样本的普遍性。
至少，如果选择性地使用一些初始探索性因子分析来减少分析中的变量数量，则应使用保持样本重新评估后续因子解决方案。

其次，简单的结构并不总是要求良好的因子分析解决方案。
请记住，探索性因素分析的目标是让数据表明底层结构。
表现出简单结构的解决方案更容易理解，但并不总是真实的样子。